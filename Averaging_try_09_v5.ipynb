{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> \"My words always get me into troubles. And if not my words, it is my facial expressions.\" \n",
    "-- Manasa Rao\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "The ability to recognize facial expressions automatically enables novel applications in human-computer interaction\n",
    "and other areas including, but not limited to, human behavior understanding, detection of mental disorders, and synthetic human expressions. Consequently, there has been active research in this field, with several recent works utilizing Convolutional\n",
    "Neural Networks (**CNN**s) for feature extraction and inference.\n",
    "\n",
    "<br/>\n",
    "In this tutorial we implement and analye a **CNN** based model for recognising Facial Expressions. The model is largely based on this paper [Image based Static Facial Expression Recognition with Multiple Deep Network Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/icmi2015_ChaZhang.pdf). To further improve the accuacy, we will modify the model based on our learning from other papers. \n",
    "\n",
    "<br/>We have tried differnt apporaches in this paper and published the results for the user . We have also compared our results to the reported SOTA (state-of-the-art) metrics of Facial Expression Recognition test results. In the end we also suggest a few probable methods to further improve the accuracy.\n",
    "\n",
    "*Tony James*\n",
    "[@tonyissacjames](https://github.com/tonyissacjames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Objective of the tutorial\n",
    "\n",
    "Get  a score of 0.69 or more for this [Kaggle challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/leaderboard).<br/>\n",
    "The challenge is no longer active so we can use the same data set(**fer2013**) used for the Kaggle competition to train and evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 About the data set\n",
    "\n",
    "We are using **fer2013** dataset for training, validation and testing.\n",
    "<br/>This data set can be downloaded from this Kaggle challenge https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/leaderboard\n",
    "<br/>This data set has images of 7 different facial expressions expressing Anger, Disgust, Fear, Happy, Neutral, Sad, Suprise.\n",
    "<br/>\n",
    "<br/>\n",
    "This data set has there categories namely Training, PublicTest, PrivateTest. <br/>**Kaggle** has given the Training category for training the models and used PublicTest for their Public Leader Board evaluation. PrivateTest was not known to users until the challenge was closed and it was used for Private Leader Board evaluation. Once this Competition was closed, **Kaggle** released all there categories to public in **fer2013.csv** file. \n",
    "<br/>We will use Training category for Training, PublicTest for Validation and PrivateTest for Testing our model\n",
    "\n",
    "\n",
    "- Train: 32181 images\n",
    "- Validation: 3589 images\n",
    "- Test : 3589 images\n",
    "- Images are of resolution 48x48 and singel channel\n",
    "- Number of Classes: 7\n",
    " - 0: Anger\n",
    " - 1: Disgust\n",
    " - 2: Fear\n",
    " - 3: Happy\n",
    " - 4: Neutral\n",
    " - 5: Sad\n",
    " - 6: Suprise\n",
    " \n",
    "<br/>\n",
    "\n",
    "- We have written few helper functions in **HelperDataProcessing** class to load and preprocess the data and also reshape the data to our needs, use them to save time and some trouble.\n",
    " - The data provided in the **fer2013.csv** file is of list format, our helper functions can convert it to 48x48 numpy array format\n",
    " - LamUong (https://github.com/LamUong/) have identifed some bad training data, and the helper funcions eliminate them while loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 How do we proceed \n",
    "\n",
    "### Follow the  Steps below\n",
    "\n",
    "1. Read Zhiding Yu's & Cha Zhang's paper [1] [Image based Static Facial Expression Recognition with Multiple Deep Network Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/icmi2015_ChaZhang.pdf)\n",
    "    - **It is very important to understand this paper**\n",
    "    - We are going to implement the **CNN** model and methods in this paper\n",
    "<br/>\n",
    "<br/>\n",
    "2. (**Optional**) Go through this repo - [https://github.com/LamUong/FacialExpressionRecognition](https://github.com/LamUong/FacialExpressionRecognition)\n",
    "    - it has lot of helper functions to read data\n",
    "    - It has code Simple Average Ensemble method\n",
    "    - We have build our helper functions using helper functions from this repo (this saved us lot of trouble and time()\n",
    "<br/>\n",
    "<br/>\n",
    "3. Do Exploratory data analysis \n",
    "    - and build some intution about the data\n",
    "    - identify any possibble problem in the dataset and plan for mitigating them them\n",
    "<br/>\n",
    "<br/>\n",
    "4. Build a NN model based on Zhiding Yu's paper [1]\n",
    "    - Use the basic **CNN** model given in the paper as a reference design\n",
    "    - Try to improve the model by \n",
    "       - adding more layers\n",
    "       - using different optimiser\n",
    "       - using different cost function etc.\n",
    "    - Use randomized peturbation of training data (Data Augmentation)\n",
    "       - The ensemble models use randomized peturbation to generate multiple outputs. So we can use same petrubations for training as well\n",
    "    - Get as high testing accuracy as possible\n",
    "<br/>\n",
    "<br/>    \n",
    "5. Evaluate the model's performance on Private Leader Board data\n",
    "<br/>\n",
    "<br/> \n",
    "6. Analyse the results of the stand alone model\n",
    "    - print confusion matrix\n",
    "    - plot validation/training accuracy/loss\n",
    "    - decide what can be further done\n",
    "<br/>\n",
    "<br/> \n",
    "7. Build Multiple Network Learning Models \n",
    "    - On top of **CNN** we have added a multiple network learning framework to ensemble multiple models. Different ensemble models can further increase performance. Two such models we have implemented are given below.\n",
    "        - **Simple Average Ensembling**\n",
    "            - here we give randomly perturbed version of the image to **CNN** network. Final ouput is the average of all the ouput of the **CNN** network.\n",
    "        - **Optimal Hinge Loss Ensembling**\n",
    "            - here we give randomly perturbed version of the image to **CNN** network. And all such ouputs are fed into another **NN** which is optimised for Hinge Loss. The **NN** calculates the final output.\n",
    "        - **Optimal Mean Squred Error(MSE) Ensembling**\n",
    "            - here we give randomly perturbed version of the image to **CNN** network. And all such ouputs are fed into another **NN** which is optimised for Mean Squred Error (MSE) Loss. The **NN** calculates the final output.\n",
    "<br/>\n",
    "<br/> \n",
    "8. Compare the results of different approaches on Private Leader Board data\n",
    "    - Choose the one with highest tesing accuracy\n",
    "        - and hope it achieves 0.69 or more on **fer2013**\n",
    "<br/>\n",
    "<br/> \n",
    "9. Write down the Conclusion \n",
    "    - compare the results with reported SOTA performance\n",
    "    - few probable methods to further improve the accuracy\n",
    "    - (**Optional**) few observations which might help the reader in the futrue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 References\n",
    "[1] [Image based Static Facial Expression Recognition with Multiple Deep Network Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/icmi2015_ChaZhang.pdf)\n",
    "<br/>\n",
    "[2] [Facial Emotion Recognition in Real Time](http://cs231n.stanford.edu/reports/2016/pdfs/022_Report.pdf)\n",
    "<br/>\n",
    "[2] [Facial Expression Recognition using Convolutional Neural Networks: State of the Art](https://arxiv.org/abs/1612.02903)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Results\n",
    "### Accuracy of different approaches are given below\n",
    "1. Stand-alone **CNN** model:\n",
    "2. Multiple Network Learning Models - **CNN** with Simple Average Ensembling:\n",
    "3. Multiple Network Learning Models - **CNN** with Hinge Loss Ensembling:\n",
    "4. Multiple Network Learning Models - **CNN** with Mean Squred Error(MSE) Ensembling:\n",
    "<br/>\n",
    "<br/>\n",
    "Maximum achieved accuracy of our implementation for **fer2013** dataset:  **0.7035**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Conclusion\n",
    "\n",
    "Ususlly **Conclusion** comes at the end. But we have put at the top so it can be looked up easily.\n",
    "<br/>\n",
    "<br/>\n",
    "In this tutorial we have tried different **CNN** based approach for Facial Expression Recognition.<br>\n",
    "It is observed that **Multiple Network Learning Models - CNN with Mean Squred Error(MSE) Ensembling** gives the maximum performace on **fer2013** data set.<br> **CNN with Mean Squred Error(MSE) Ensembling** gives an accuracy of **0.7035**. This supasses our intial expectation of 0.69. <br>\n",
    "So we have attained our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Comparing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Suggestion to further imporve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Some observaions\n",
    " - the training and validaton class distribution is very different\n",
    " - the training class distribution is not balanced\n",
    " - the validation accuracy and private leader board accuracy are very close\n",
    "     - their class distribution is also very close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Instruction to run the tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make sure the dataset **fer2013.csv** and **badtrainingdata.txt** is available in the same folder where this ipython note book file is placed\n",
    "- check Cell **4.5 Global settings** and update the settings according to your need\n",
    "- start running the code from Cell **4 Code Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4  Code Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.optimizers import Adadelta ,SGD \n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers import deserialize as layer_from_config\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger \n",
    "from keras.callbacks import History \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight \n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Helper functions- to load and pre-process the data\n",
    "\n",
    "These helper functions are taken from https://github.com/LamUong/FacialExpressionRecognition/blob/master/dataprocessing.py\n",
    "These functins saves us a lot of trouble. The image data is given in a csv (fer2013.csv) file. And these functions helps us to easyly load tesing and training data. The author has also identified few bad training data and listed out in \"badtrainingdata.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- get_formatted_train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import _pickle as cPickle\n",
    "import numpy\n",
    "import csv\n",
    "import scipy.misc\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelperDataProcessing:\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_matrix(matrix):\n",
    "        vector = matrix.flatten(1)\n",
    "        vector = vector.reshape(1, len(vector))\n",
    "        return vector\n",
    "    \n",
    "    @staticmethod\n",
    "    def zca_whitening(inputs):\n",
    "        sigma = np.dot(inputs, inputs.T)/inputs.shape[1] #Correlation matrix\n",
    "        U,S,V = np.linalg.svd(sigma) #Singular Value Decomposition\n",
    "        epsilon = 0.1                #Whitening constant, it prevents division by zero\n",
    "        ZCAMatrix = np.dot(np.dot(U, np.diag(1.0/np.sqrt(np.diag(S) + epsilon))), U.T)                     #ZCA Whitening matrix\n",
    "        return np.dot(ZCAMatrix, inputs)   #Data whitening\n",
    "\n",
    "    @staticmethod\n",
    "    def global_contrast_normalize(X, scale=1., subtract_mean=True, use_std=True,\n",
    "                                  sqrt_bias=10, min_divisor=1e-8):\n",
    "\n",
    "        \"\"\"\n",
    "        __author__ = \"David Warde-Farley\"\n",
    "        __copyright__ = \"Copyright 2012, Universite de Montreal\"\n",
    "        __credits__ = [\"David Warde-Farley\"]\n",
    "        __license__ = \"3-clause BSD\"\n",
    "        __email__ = \"wardefar@iro\"\n",
    "        __maintainer__ = \"David Warde-Farley\"\n",
    "        .. [1] A. Coates, H. Lee and A. Ng. \"An Analysis of Single-Layer\n",
    "           Networks in Unsupervised Feature Learning\". AISTATS 14, 2011.\n",
    "           http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf\n",
    "        \"\"\"\n",
    "        assert X.ndim == 2, \"X.ndim must be 2\"\n",
    "        scale = float(scale)\n",
    "        assert scale >= min_divisor\n",
    "\n",
    "        mean = X.mean(axis=1)\n",
    "        if subtract_mean:\n",
    "            X = X - mean[:, numpy.newaxis]  \n",
    "        else:\n",
    "            X = X.copy()\n",
    "        if use_std:\n",
    "            ddof = 1\n",
    "            if X.shape[1] == 1:\n",
    "                ddof = 0\n",
    "            normalizers = numpy.sqrt(sqrt_bias + X.var(axis=1, ddof=ddof)) / scale\n",
    "        else:\n",
    "            normalizers = numpy.sqrt(sqrt_bias + (X ** 2).sum(axis=1)) / scale\n",
    "        normalizers[normalizers < min_divisor] = 1.\n",
    "        X /= normalizers[:, numpy.newaxis]  # Does not make a copy.\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def ZeroCenter(data):\n",
    "        data = data - numpy.mean(data,axis=0)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(arr):\n",
    "        for i in range(3):\n",
    "            minval = arr[...,i].min()\n",
    "            maxval = arr[...,i].max()\n",
    "            if minval != maxval:\n",
    "                arr[...,i] -= minval\n",
    "                arr[...,i] *= (255.0/(maxval-minval))\n",
    "        return arr\n",
    "\n",
    "    @staticmethod\n",
    "    def Flip(data):\n",
    "        dataFlipped = data[..., ::-1].reshape(2304).tolist()\n",
    "        return dataFlipped\n",
    "\n",
    "    @staticmethod\n",
    "    def Roated15Left(data):\n",
    "        num_rows, num_cols = data.shape[:2]\n",
    "        rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 30, 1)\n",
    "        img_rotation = cv2.warpAffine(data, rotation_matrix, (num_cols, num_rows))\n",
    "        return img_rotation.reshape(2304).tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def Roated15Right(data):\n",
    "        num_rows, num_cols = data.shape[:2]\n",
    "        rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), -30, 1)\n",
    "        img_rotation = cv2.warpAffine(data, rotation_matrix, (num_cols, num_rows))\n",
    "        return img_rotation.reshape(2304).tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def Zoomed(data):\n",
    "        datazoomed = scipy.misc.imresize(data,(60,60))\n",
    "        datazoomed = datazoomed[5:53,5:53]\n",
    "        datazoomed = datazoomed.reshape(2304).tolist()\n",
    "        return datazoomed\n",
    "\n",
    "    @staticmethod\n",
    "    def shiftedUp20(data):\n",
    "        translated = imutils.translate(data, 0, -5)\n",
    "        translated2 = translated.reshape(2304).tolist()\n",
    "        return translated2\n",
    "    \n",
    "    @staticmethod\n",
    "    def shiftedDown20(data):\n",
    "        translated = imutils.translate(data, 0, 5)\n",
    "        translated2 = translated.reshape(2304).tolist()\n",
    "        return translated2\n",
    "\n",
    "    @staticmethod\n",
    "    def shiftedLeft20(data):\n",
    "        translated = imutils.translate(data, -5, 0)\n",
    "        translated2 = translated.reshape(2304).tolist()\n",
    "        return translated2\n",
    "    \n",
    "    @staticmethod\n",
    "    def shiftedRight20(data):\n",
    "        translated = imutils.translate(data, 5, 0)\n",
    "        translated2 = translated.reshape(2304).tolist()\n",
    "        return translated2\n",
    "    \n",
    "    @staticmethod\n",
    "    def outputImage(pixels,number):\n",
    "        data = pixels\n",
    "        name = str(number)+\"output.jpg\" \n",
    "        scipy.misc.imsave(name, data)\n",
    "\n",
    "    @staticmethod\n",
    "    def Zerocenter_ZCA_whitening_Global_Contrast_Normalize(list):\n",
    "        Intonumpyarray = numpy.asarray(list)\n",
    "        data = Intonumpyarray.reshape(48,48)\n",
    "        data2 = HelperDataProcessing.ZeroCenter(data)\n",
    "        data3 = HelperDataProcessing.zca_whitening(HelperDataProcessing.flatten_matrix(data2)).reshape(48,48)\n",
    "        data4 = HelperDataProcessing.global_contrast_normalize(data3)\n",
    "        data5 = numpy.rot90(data4,3)\n",
    "        return data5\n",
    "\n",
    "    @staticmethod\n",
    "    def load_test_data():\n",
    "        f = open('fer2013.csv')\n",
    "        csv_f = csv.reader(f)\n",
    "        test_set_x =[]\n",
    "        test_set_y =[]\n",
    "        for row in csv_f:  \n",
    "            if str(row[2]) == \"PrivateTest\" :\n",
    "                test_set_y.append(int(row[0]))\n",
    "                temp_list = []\n",
    "                for pixel in row[1].split( ):\n",
    "                    temp_list.append(int(pixel))\n",
    "                data = HelperDataProcessing.Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list)\n",
    "                test_set_x.append(data)\n",
    "        return test_set_x, test_set_y\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data():\n",
    "\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        val_x =[]\n",
    "        val_y =[]\n",
    "\n",
    "        with open(\"badtrainingdata.txt\", \"r\") as text:\n",
    "            ToBeRemovedTrainingData = []\n",
    "            for line in text:\n",
    "                ToBeRemovedTrainingData.append(int(line))\n",
    "        number = 0\n",
    "\n",
    "        f = open('fer2013.csv')\n",
    "        csv_f = csv.reader(f)\n",
    "\n",
    "        for row in csv_f:   \n",
    "            number+= 1\n",
    "            if number not in ToBeRemovedTrainingData:\n",
    "\n",
    "                if str(row[2]) == \"Training\" or str(row[2]) == \"PublicTest\" :\n",
    "                    temp_list = []\n",
    "\n",
    "                    for pixel in row[1].split( ):\n",
    "                        temp_list.append(int(pixel))\n",
    "\n",
    "                    data = HelperDataProcessing.Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list)\n",
    "                    train_y.append(int(row[0]))\n",
    "                    train_x.append(data.reshape(2304).tolist())\n",
    "\n",
    "                elif str(row[2]) == \"PrivateTest\":\n",
    "                    temp_list = []\n",
    "\n",
    "                    for pixel in row[1].split( ):\n",
    "                        temp_list.append(int(pixel))\n",
    "\n",
    "                    data = HelperDataProcessing.Zerocenter_ZCA_whitening_Global_Contrast_Normalize(temp_list)\n",
    "                    val_y.append(int(row[0]))\n",
    "                    val_x.append(data.reshape(2304).tolist())\n",
    "\n",
    "        return train_x, train_y, val_x, val_y\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_formatted_train_val_data():\n",
    "\n",
    "        X_train, y_train, X_val, y_val  = HelperDataProcessing.load_data()\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            X_train[i] = (array(X_train[i])).reshape(48, 48, 1)\n",
    "\n",
    "        X_train= array(X_train)\n",
    "\n",
    "        for i in range(len(X_val)):\n",
    "            X_val[i] = (array(X_val[i])).reshape(48, 48, 1)\n",
    "\n",
    "        X_val= array(X_val)\n",
    "\n",
    "        y_train = array(y_train)\n",
    "        y_val   = array(y_val)\n",
    "\n",
    "        # Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "        Y_train = np_utils.to_categorical(y_train, 7)\n",
    "        Y_val = np_utils.to_categorical(y_val, 7)\n",
    "\n",
    "        return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "    @staticmethod\n",
    "    def get_formatted_private_test_data():\n",
    "\n",
    "        X_private_test, y_private_test  = HelperDataProcessing.load_test_data()\n",
    "\n",
    "        for i in range(len(X_private_test)):\n",
    "            X_private_test[i] = (array(X_private_test[i])).reshape(48, 48, 1)\n",
    "\n",
    "        X_private_test= array(X_private_test)\n",
    "        Y_private_test = np_utils.to_categorical(y_private_test, 7)\n",
    "\n",
    "        return X_private_test, Y_private_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Helper functions to test model on peturbed data\n",
    "These helper functions are taken from https://github.com/LamUong/FacialExpressionRecognition/blob/master/averagemethod.py and modified according to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(number,test_set_x,model):\n",
    "    data_p = []\n",
    "    for data in test_set_x:\n",
    "        if number ==0:\n",
    "            data_p.append(HelperDataProcessing.Flip(data))\n",
    "        elif number ==1:\n",
    "            data_p.append(HelperDataProcessing.Roated15Left(data))\n",
    "        elif number ==2:\n",
    "            data_p.append(HelperDataProcessing.Roated15Right(data))\n",
    "        elif number ==3:\n",
    "            data_p.append(HelperDataProcessing.shiftedUp20(data))\n",
    "        elif number ==4:\n",
    "            data_p.append(HelperDataProcessing.shiftedDown20(data))\n",
    "        elif number ==5:\n",
    "            data_p.append(HelperDataProcessing.shiftedLeft20(data))\n",
    "        elif number ==6:\n",
    "            data_p.append(HelperDataProcessing.shiftedRight20(data))\n",
    "        elif number ==7:\n",
    "            data_p.append(data)\n",
    "    \n",
    "    #Covert the data to Numpy format\n",
    "    for i in range(len(data_p)):\n",
    "        data_p[i] = (array(data_p[i])).reshape(48, 48, 1)\n",
    "        \n",
    "    X_private_test= array(data_p)\n",
    "            \n",
    "    proba = model.predict_proba(X_private_test)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Helper functions to dispaly and analyze  data\n",
    "These helper functions<br/>\n",
    " \n",
    "- plot_confusion_matrix: helps us to plot the confusion matrix. <br/> It is taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html. <br/>The same page has some sample examples on how to use this function\n",
    "\n",
    "- plot_histogram: helps to plot the histogram of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm) #to print in text if needed\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_histogram(lst_data, class_labels, ylabel='None', title='None'):\n",
    "    data = pd.Series(lst_data)\n",
    "    distribution = data.value_counts(sort=False)\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    \n",
    "    plt.bar(y_pos, distribution, align='center', alpha=0.8)\n",
    "    plt.xticks(y_pos, class_labels)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelperPlotLogs():\n",
    "    def __init__(self, *file_names):\n",
    "        self.__file_names  = []\n",
    "        self.__data_frames = []\n",
    "        self.__design      = ['r--', 'b--', 'g--', 'p--']\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            self.__file_names.append(file_name)\n",
    " \n",
    "        for file_name in self.__file_names:\n",
    "            self.__data_frames.append(pd.read_csv(file_name))\n",
    "            print(self.__data_frames[-1].columns)\n",
    "\n",
    "        #print(self.__data_frames[0].columns)\n",
    "        #print(self.__data_frames[1].columns)\n",
    "        columns = len(self.__data_frames[0].columns)\n",
    "        \n",
    "        for df in  self.__data_frames:\n",
    "            if columns- len(df.columns):\n",
    "                print(\"Header not matching\")\n",
    "        \n",
    "    def plot_aganist(self, val):\n",
    "        file_index = 1\n",
    "        for df, design  in  zip(self.__data_frames, self.__design):\n",
    "            plt.plot(df[val], design, label='file_'+str(file_index))\n",
    "            file_index += 1\n",
    "        \n",
    "        plt.ylabel(val)\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "            \n",
    "    def plot_same(self, *vals, file_number=0):\n",
    "        for val, design in zip(vals, self.__design):\n",
    "            plt.plot(self.__data_frames[file_number][val], design, label=val)\n",
    "            \n",
    "        plt.ylabel(vals)\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_same_aganist(self, *vals, file_numbers=[0]):\n",
    "        rows  = 1\n",
    "        cols  = len(file_numbers)\n",
    "        file_index = 1\n",
    "        #plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "        \n",
    "        for file_number in file_numbers:            \n",
    "            for val, design in zip(vals, self.__design):\n",
    "                #plt.subplot(rows, cols, file_index)\n",
    "                plt.plot(self.__data_frames[file_number][val], design, label=val)\n",
    "            file_index += 1\n",
    "            \n",
    "            plt.ylabel(vals)\n",
    "            plt.legend(loc='best')      \n",
    "            plt.show()\n",
    "\n",
    "    @staticmethod        \n",
    "    def save_dict_to_csv(file_name, dict_list_1, write_header=True, write_mode='w'):\n",
    "        \"\"\"If give two dictionaries this function will \n",
    "            put them in a file_name.csv file\"\"\"\n",
    "\n",
    "        if len(dict_list_1) != 0:\n",
    "            print(\"Lengths of list not matching\")\n",
    "            return\n",
    "\n",
    "        fieldnames = []\n",
    "        dict1 = dict_list_1[0]\n",
    "\n",
    "        for field in dict1:\n",
    "            fieldnames.append(field)\n",
    "\n",
    "        with open(file_name, write_mode) as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for dict1 in dict_list_1:\n",
    "                writer.writerow(dict1)\n",
    "                \n",
    "    @staticmethod\n",
    "    def save_2dict_to_csv(file_name, dict_list_1, dict_list_2, write_header=True, write_mode='w'):\n",
    "        \"\"\"If give two dictionaries this function will \n",
    "            put them in a file_name.csv file\n",
    "            typical usage:\n",
    "            save_2dict_to_csv(file_prefix+ r\"my_logger.csv\", my_logger.logs, my_logger.optimizer_parms)\"\"\"\n",
    "\n",
    "        #dict_list_1 = copy.deepcopy(dict_list_1_tmp)\n",
    "\n",
    "        if len(dict_list_1) != len(dict_list_2):\n",
    "            print(\"Lengths of list not matching\")\n",
    "            return\n",
    "\n",
    "        fieldnames = []\n",
    "        dict1 = dict_list_1[0]\n",
    "        dict2 = dict_list_2[0]\n",
    "\n",
    "        for field in dict1:\n",
    "            fieldnames.append(field)\n",
    "\n",
    "        for field in dict2:\n",
    "            fieldnames.append(field)\n",
    "\n",
    "        with open(file_name, write_mode) as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for dict1, dict2 in zip(dict_list_1, dict_list_2):\n",
    "                #dict1_c = copy.deepcopy(dict1)\n",
    "                #dict2_c = copy.deepcopy(dict2)\n",
    "                dict1.update(dict2)\n",
    "                writer.writerow(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class My_Logger(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.global_epoch = 0\n",
    "        self.optimizer_parms = []\n",
    "        self.logs = []\n",
    "        self.wrote_header = False\n",
    "        \n",
    "    def on_train_begin(self, logs={}):    \n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self,  epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer_name = self.model.optimizer\n",
    "        optimizer_param =  copy.deepcopy((self.model.optimizer.get_config()))\n",
    "        #print(optimizer_param)\n",
    "        optimizer_param['g_epoch']  = self.global_epoch\n",
    "        optimizer_param['l_epoch']  = epoch\n",
    "        \n",
    "        self.optimizer_parms.append(optimizer_param)\n",
    " \n",
    "        log_dict = copy.deepcopy(logs)\n",
    "\n",
    "        log_dict['opt_name'] = optimizer_name\n",
    "        self.logs.append(log_dict)\n",
    "        #print(self.logs)\n",
    "        \n",
    "        self.global_epoch   = self.global_epoch + 1\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Global settings\n",
    "\n",
    "Preferably this should be the only place where we should be changing global settings for this tutorial\n",
    "\n",
    "**Global Settings**:\n",
    "- `file_prefix`: All files (logs, saved model) fill have this string as the prefix. Useful when we run multiple trials.\n",
    "- `do_sub_sampling_of_input`: if set to **True** then training and validation happens on only 25% of train and validation samples of fer2013.csv this helps us to do fast training to check the feasibility of a model. if **False** then we use the full data set for Training and Validation\n",
    "- `Facial_Expressions`: list of **Facial Expressions** and the index of each Facial Expression in the list is same as the class number used in fer2013.csv. <br/> For example index of **'Angry'** is **0** and the class number used for angry faces is also **0**. <br/> Index of **'Happy'** is **3** and the class number used for happy faces is also **3**. <br/> In fer2013.csv class number  0 is for Angry, 1 for Disgust, 2 for Fear, 3 for Happy, 4 for Neutral, 5 for Sad and 6 for Surprise.\n",
    "- `ModelPerformance`: is a pandas DataFrame which we use to keep track of different methods and it's accuracies.<br> We can add data to the DataFrame like this, model_performance.loc[len(model_performance)] = ['method1',  69.7]\n",
    "- `do_data_appending`: if set to **True** then the training data is appended by a copy of itself. While we do data augmentation using ImageDataGenerator, the input samples gets modified. ImageDataGenerator randomly picks up samples(based on some probability) and modifies it. Doubling the training data samples make sure that orginal samples get more weightage in training. Because it is unlikely that ImageDataGenerator picks up the same sample again. if set to **False** then no appeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix               = \"Final_Subm_FER_Tony_v1_\"  \n",
    "do_sub_sampling_of_input  = False\n",
    "Facial_Expressions        = ['Anger', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Suprise']\n",
    "ModelPerformance          = pd.DataFrame(columns=['method_name','accuracy'])\n",
    "do_data_appending         = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exploratory data analysis\n",
    "The goal is to build some intuition around the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load Training and Validation data (Public Leader Board data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val = HelperDataProcessing.get_formatted_train_val_data() #usually take some time to load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load Test data (Private Leader Board data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_private_test, Y_private_test = HelperDataProcessing.get_formatted_private_test_data() #usually takes some time to load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the dimensions of train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32181, 48, 48, 1) (32181, 7) (3589, 48, 48, 1) (3589, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3589, 48, 48, 1) (3589, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_private_test.shape, Y_private_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Check the class distribution\n",
    "\n",
    "Checking the data distribution requires loading the facial expresson classes, not the one hot encoded version of class.\n",
    "So we need to load the labels again for testing and validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val = HelperDataProcessing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Class Distribution\n",
      " 0    4441\n",
      "1     491\n",
      "2    4574\n",
      "3    8088\n",
      "4    5460\n",
      "5    3575\n",
      "6    5552\n",
      "dtype: int64\n",
      "Validation Data Class Distribution\n",
      " 0    491\n",
      "2    528\n",
      "4    594\n",
      "6    626\n",
      "1     55\n",
      "3    879\n",
      "5    416\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Class Distribution\\n\",pd.Series(y_train).value_counts(sort=False))\n",
    "print(\"Validation Data Class Distribution\\n\",pd.Series(y_val).value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(y_train, Facial_Expressions, ylabel='Frequency',title='Facial Expression Class Distribution in Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_histogram(y_val, Facial_Expressions, ylabel='Frequency',title='Facial Expression Class Distribution in Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Observations from inspecting the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle the class distribution  imbalance\n",
    "\n",
    "- We can see that there is an imbalance in the class distribution in Training data set. Also, Training and Validaton data set distribution is totally different too. This means in Testing/real world scenarios we can expect any kind of distribution.\n",
    "<br/>\n",
    "<br/>\n",
    "- We have 3 options here\n",
    " - use class_weights argument in model.fit() and make the model learn more from minority classes\n",
    " - reduce the majority class sized to make the distribution balanced (not a very good one, as we don't have much data)\n",
    " - data augmentation for minority classes\n",
    "<br/>\n",
    "<br/>\n",
    "- We will try \n",
    " - using class_weights argument, in model.fit() \n",
    " - and if time allows data augmentation of minority class aswell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying the class weights\n",
    "The weights should be such that <br/>\n",
    "$w_0*n_0 = w_1*n_1 =... == w_6*n_6$\n",
    "<br/>\n",
    "where, $w_i$ is class weights and $n_i$ is number of samples in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   4597.29\n",
       "1   4597.29\n",
       "2   4597.29\n",
       "3   4597.29\n",
       "4   4597.29\n",
       "5   4597.29\n",
       "6   4597.29\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights* pd.Series(y_train).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Build and Train the model\n",
    "\n",
    "The model is based on paper [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all custom cost functions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rmse_fer\n",
    "This function is used to monitor the root mean squared error in the non-predicted classes<br>\n",
    "This will help us to know the strength of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_fer(y_true, y_pred):\n",
    "        \n",
    "    ce_loss = 0.0\n",
    "    \n",
    "    rmse_loss = K.sqrt(K.mean(K.square((y_pred - y_true)*(1 - y_true)), axis=-1))\n",
    "    \n",
    "    return rmse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all the callback functions needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback for saving the best model\n",
    "\n",
    "filepath = file_prefix + r\".best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_log_file = file_prefix + r\".csv\"\n",
    "csv_logger   = keras.callbacks.CSVLogger(csv_log_file, separator=',', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.callbacks import ReduceLROnPlateau \n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_logger  = My_Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list = [checkpoint, history, csv_logger, my_logger, clr_triangular]\n",
    "callbacks_list = [checkpoint, history, csv_logger, my_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session() #Useful to clear the session from GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intitilisation for different optimizers\n",
    "1. sgd      = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "2. adadelta = Adadelta(lr=0.1, rho=0.95, epsilon=1e-08)\n",
    "3. rmsprop  = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "4. adagrad  = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "5. adam     = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sub_sampling_of_input:\n",
    "    X_, X_train, Y_, Y_train = cross_validation.train_test_split(X_train, Y_train, test_size=0.25, random_state=0)\n",
    "    X_, X_val,  Y_, Y_val    = cross_validation.train_test_split(X_val, Y_val, test_size=0.25, random_state=0)\n",
    "    print(\"After SubSampling\", X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_data_appending:\n",
    "    X_train = np.append(X_train, X_train,axis=0)\n",
    "    Y_train = np.append(Y_train, Y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,    # randomly flip images\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate():\n",
    "    img_rows, img_cols = 48, 48\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, kernel_size=(5, 5), padding='same', input_shape=(img_rows, img_cols, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Dropout(0.3))\n",
    "        \n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization()) \n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(512, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #ada = Adadelta(lr=0.1, rho=0.95, epsilon=1e-08)\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #adam = keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy',rmse_fer])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 128)       3328      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 11, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 11, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 512)         590336    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              9439232   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 7175      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 13,782,791\n",
      "Trainable params: 13,780,487\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model_cnn # if model exist delete it\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "model_cnn = model_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 7\n",
    "if do_sub_sampling_of_input:\n",
    "    nb_epcohs = 200\n",
    "else:\n",
    "    nb_epcohs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/200\n",
      "503/503 [==============================] - 81s 162ms/step - loss: 1.7902 - acc: 0.2651 - rmse_fer: 0.1479 - val_loss: 1.6246 - val_acc: 0.3408 - val_rmse_fer: 0.1435\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.34076, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 2/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.6248 - acc: 0.3523 - rmse_fer: 0.1434 - val_loss: 1.5415 - val_acc: 0.3970 - val_rmse_fer: 0.1435\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.34076 to 0.39705, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 3/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.4974 - acc: 0.4152 - rmse_fer: 0.1403 - val_loss: 1.3563 - val_acc: 0.4675 - val_rmse_fer: 0.1391\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.39705 to 0.46754, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 4/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.4148 - acc: 0.4553 - rmse_fer: 0.1378 - val_loss: 1.3405 - val_acc: 0.4882 - val_rmse_fer: 0.1367\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.46754 to 0.48816, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 5/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.3658 - acc: 0.4758 - rmse_fer: 0.1369 - val_loss: 1.2969 - val_acc: 0.4993 - val_rmse_fer: 0.1405\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.48816 to 0.49930, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 6/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.3167 - acc: 0.4972 - rmse_fer: 0.1356 - val_loss: 1.2136 - val_acc: 0.5364 - val_rmse_fer: 0.1319\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.49930 to 0.53636, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 7/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.2789 - acc: 0.5130 - rmse_fer: 0.1342 - val_loss: 1.1938 - val_acc: 0.5339 - val_rmse_fer: 0.1306\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.53636\n",
      "Epoch 8/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.2522 - acc: 0.5220 - rmse_fer: 0.1333 - val_loss: 1.1751 - val_acc: 0.5442 - val_rmse_fer: 0.1331\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.53636 to 0.54416, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 9/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.2271 - acc: 0.5316 - rmse_fer: 0.1325 - val_loss: 1.1305 - val_acc: 0.5678 - val_rmse_fer: 0.1301\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.54416 to 0.56785, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 10/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.2050 - acc: 0.5428 - rmse_fer: 0.1318 - val_loss: 1.1363 - val_acc: 0.5606 - val_rmse_fer: 0.1322\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.56785\n",
      "Epoch 11/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.1857 - acc: 0.5517 - rmse_fer: 0.1308 - val_loss: 1.0568 - val_acc: 0.5952 - val_rmse_fer: 0.1263\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.56785 to 0.59515, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 12/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.1674 - acc: 0.5586 - rmse_fer: 0.1298 - val_loss: 1.1019 - val_acc: 0.5765 - val_rmse_fer: 0.1306\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.59515\n",
      "Epoch 13/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.1525 - acc: 0.5616 - rmse_fer: 0.1293 - val_loss: 1.0370 - val_acc: 0.5985 - val_rmse_fer: 0.1236\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.59515 to 0.59850, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 14/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.1399 - acc: 0.5686 - rmse_fer: 0.1288 - val_loss: 1.0515 - val_acc: 0.6041 - val_rmse_fer: 0.1253\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.59850 to 0.60407, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 15/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.1255 - acc: 0.5743 - rmse_fer: 0.1280 - val_loss: 1.0634 - val_acc: 0.5957 - val_rmse_fer: 0.1248\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60407\n",
      "Epoch 16/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.1144 - acc: 0.5794 - rmse_fer: 0.1274 - val_loss: 1.0209 - val_acc: 0.6060 - val_rmse_fer: 0.1213\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.60407 to 0.60602, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 17/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.1061 - acc: 0.5836 - rmse_fer: 0.1270 - val_loss: 1.0178 - val_acc: 0.6133 - val_rmse_fer: 0.1210\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.60602 to 0.61326, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 18/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0979 - acc: 0.5866 - rmse_fer: 0.1265 - val_loss: 0.9973 - val_acc: 0.6155 - val_rmse_fer: 0.1210\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.61326 to 0.61549, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 19/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0860 - acc: 0.5894 - rmse_fer: 0.1260 - val_loss: 1.0161 - val_acc: 0.6138 - val_rmse_fer: 0.1240\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.61549\n",
      "Epoch 20/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0778 - acc: 0.5936 - rmse_fer: 0.1257 - val_loss: 0.9893 - val_acc: 0.6205 - val_rmse_fer: 0.1234\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.61549 to 0.62051, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 21/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0678 - acc: 0.5967 - rmse_fer: 0.1251 - val_loss: 0.9828 - val_acc: 0.6244 - val_rmse_fer: 0.1219\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.62051 to 0.62441, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 22/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0589 - acc: 0.5998 - rmse_fer: 0.1246 - val_loss: 0.9869 - val_acc: 0.6333 - val_rmse_fer: 0.1202\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.62441 to 0.63332, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 23/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0524 - acc: 0.6028 - rmse_fer: 0.1242 - val_loss: 0.9794 - val_acc: 0.6233 - val_rmse_fer: 0.1211\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.63332\n",
      "Epoch 24/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0463 - acc: 0.6055 - rmse_fer: 0.1240 - val_loss: 0.9794 - val_acc: 0.6244 - val_rmse_fer: 0.1241\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.63332\n",
      "Epoch 25/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0397 - acc: 0.6074 - rmse_fer: 0.1238 - val_loss: 0.9664 - val_acc: 0.6330 - val_rmse_fer: 0.1195\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.63332\n",
      "Epoch 26/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0340 - acc: 0.6106 - rmse_fer: 0.1234 - val_loss: 0.9719 - val_acc: 0.6289 - val_rmse_fer: 0.1210\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.63332\n",
      "Epoch 27/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0259 - acc: 0.6145 - rmse_fer: 0.1228 - val_loss: 1.0077 - val_acc: 0.6216 - val_rmse_fer: 0.1270\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.63332\n",
      "Epoch 28/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0219 - acc: 0.6146 - rmse_fer: 0.1230 - val_loss: 0.9592 - val_acc: 0.6342 - val_rmse_fer: 0.1195\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.63332 to 0.63416, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 29/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0152 - acc: 0.6173 - rmse_fer: 0.1223 - val_loss: 0.9881 - val_acc: 0.6264 - val_rmse_fer: 0.1241\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.63416\n",
      "Epoch 30/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 1.0107 - acc: 0.6186 - rmse_fer: 0.1219 - val_loss: 1.0339 - val_acc: 0.6080 - val_rmse_fer: 0.1260\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.63416\n",
      "Epoch 31/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 1.0043 - acc: 0.6215 - rmse_fer: 0.1217 - val_loss: 0.9372 - val_acc: 0.6450 - val_rmse_fer: 0.1173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_acc improved from 0.63416 to 0.64503, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 32/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9980 - acc: 0.6227 - rmse_fer: 0.1215 - val_loss: 0.9704 - val_acc: 0.6297 - val_rmse_fer: 0.1215\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.64503\n",
      "Epoch 33/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9939 - acc: 0.6258 - rmse_fer: 0.1209 - val_loss: 0.9945 - val_acc: 0.6250 - val_rmse_fer: 0.1241\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.64503\n",
      "Epoch 34/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.9898 - acc: 0.6266 - rmse_fer: 0.1208 - val_loss: 0.9416 - val_acc: 0.6414 - val_rmse_fer: 0.1163\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.64503\n",
      "Epoch 35/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9802 - acc: 0.6317 - rmse_fer: 0.1201 - val_loss: 0.9419 - val_acc: 0.6456 - val_rmse_fer: 0.1183\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.64503 to 0.64558, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 36/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.9742 - acc: 0.6338 - rmse_fer: 0.1201 - val_loss: 0.9677 - val_acc: 0.6350 - val_rmse_fer: 0.1218\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.64558\n",
      "Epoch 37/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9714 - acc: 0.6341 - rmse_fer: 0.1196 - val_loss: 0.9480 - val_acc: 0.6434 - val_rmse_fer: 0.1186\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.64558\n",
      "Epoch 38/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.9686 - acc: 0.6342 - rmse_fer: 0.1195 - val_loss: 0.9545 - val_acc: 0.6461 - val_rmse_fer: 0.1194\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.64558 to 0.64614, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 39/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9637 - acc: 0.6382 - rmse_fer: 0.1194 - val_loss: 0.9539 - val_acc: 0.6358 - val_rmse_fer: 0.1229\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.64614\n",
      "Epoch 40/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9587 - acc: 0.6389 - rmse_fer: 0.1195 - val_loss: 0.9392 - val_acc: 0.6478 - val_rmse_fer: 0.1185\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.64614 to 0.64781, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 41/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9556 - acc: 0.6407 - rmse_fer: 0.1187 - val_loss: 0.9682 - val_acc: 0.6347 - val_rmse_fer: 0.1206\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.64781\n",
      "Epoch 42/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9501 - acc: 0.6415 - rmse_fer: 0.1184 - val_loss: 0.9284 - val_acc: 0.6459 - val_rmse_fer: 0.1189\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.64781\n",
      "Epoch 43/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9484 - acc: 0.6432 - rmse_fer: 0.1186 - val_loss: 0.9400 - val_acc: 0.6369 - val_rmse_fer: 0.1180\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.64781\n",
      "Epoch 44/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9425 - acc: 0.6465 - rmse_fer: 0.1179 - val_loss: 0.9038 - val_acc: 0.6656 - val_rmse_fer: 0.1154\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.64781 to 0.66565, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 45/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9413 - acc: 0.6467 - rmse_fer: 0.1179 - val_loss: 0.9347 - val_acc: 0.6459 - val_rmse_fer: 0.1177\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.66565\n",
      "Epoch 46/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.9362 - acc: 0.6486 - rmse_fer: 0.1171 - val_loss: 0.9493 - val_acc: 0.6406 - val_rmse_fer: 0.1198\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.66565\n",
      "Epoch 47/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9275 - acc: 0.6498 - rmse_fer: 0.1170 - val_loss: 0.9390 - val_acc: 0.6484 - val_rmse_fer: 0.1222\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.66565\n",
      "Epoch 48/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9314 - acc: 0.6479 - rmse_fer: 0.1176 - val_loss: 0.9093 - val_acc: 0.6534 - val_rmse_fer: 0.1162\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.66565\n",
      "Epoch 49/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9213 - acc: 0.6535 - rmse_fer: 0.1165 - val_loss: 0.9221 - val_acc: 0.6498 - val_rmse_fer: 0.1168\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.66565\n",
      "Epoch 50/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9176 - acc: 0.6547 - rmse_fer: 0.1168 - val_loss: 0.9235 - val_acc: 0.6598 - val_rmse_fer: 0.1162\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.66565\n",
      "Epoch 51/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9141 - acc: 0.6574 - rmse_fer: 0.1164 - val_loss: 0.9153 - val_acc: 0.6545 - val_rmse_fer: 0.1167\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.66565\n",
      "Epoch 52/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9145 - acc: 0.6562 - rmse_fer: 0.1165 - val_loss: 0.9025 - val_acc: 0.6645 - val_rmse_fer: 0.1141\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.66565\n",
      "Epoch 53/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9071 - acc: 0.6590 - rmse_fer: 0.1159 - val_loss: 0.9329 - val_acc: 0.6576 - val_rmse_fer: 0.1189\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.66565\n",
      "Epoch 54/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9037 - acc: 0.6596 - rmse_fer: 0.1155 - val_loss: 0.9042 - val_acc: 0.6587 - val_rmse_fer: 0.1172\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.66565\n",
      "Epoch 55/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9022 - acc: 0.6616 - rmse_fer: 0.1156 - val_loss: 0.9091 - val_acc: 0.6556 - val_rmse_fer: 0.1170\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.66565\n",
      "Epoch 56/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.9020 - acc: 0.6590 - rmse_fer: 0.1158 - val_loss: 0.9092 - val_acc: 0.6643 - val_rmse_fer: 0.1165\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.66565\n",
      "Epoch 57/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8965 - acc: 0.6629 - rmse_fer: 0.1156 - val_loss: 0.9040 - val_acc: 0.6701 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.66565 to 0.67010, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 58/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8953 - acc: 0.6626 - rmse_fer: 0.1152 - val_loss: 0.9124 - val_acc: 0.6576 - val_rmse_fer: 0.1181\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.67010\n",
      "Epoch 59/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8897 - acc: 0.6670 - rmse_fer: 0.1147 - val_loss: 0.9872 - val_acc: 0.6322 - val_rmse_fer: 0.1236\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.67010\n",
      "Epoch 60/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8865 - acc: 0.6666 - rmse_fer: 0.1146 - val_loss: 0.9046 - val_acc: 0.6551 - val_rmse_fer: 0.1156\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.67010\n",
      "Epoch 61/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8850 - acc: 0.6682 - rmse_fer: 0.1145 - val_loss: 0.9299 - val_acc: 0.6709 - val_rmse_fer: 0.1177\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.67010 to 0.67094, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 62/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8802 - acc: 0.6686 - rmse_fer: 0.1141 - val_loss: 0.8994 - val_acc: 0.6651 - val_rmse_fer: 0.1135\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.67094\n",
      "Epoch 63/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8753 - acc: 0.6713 - rmse_fer: 0.1139 - val_loss: 0.8968 - val_acc: 0.6609 - val_rmse_fer: 0.1142\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.67094\n",
      "Epoch 64/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8769 - acc: 0.6701 - rmse_fer: 0.1138 - val_loss: 0.9199 - val_acc: 0.6598 - val_rmse_fer: 0.1178\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.67094\n",
      "Epoch 65/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8727 - acc: 0.6719 - rmse_fer: 0.1133 - val_loss: 0.8940 - val_acc: 0.6640 - val_rmse_fer: 0.1141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_acc did not improve from 0.67094\n",
      "Epoch 66/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8655 - acc: 0.6756 - rmse_fer: 0.1127 - val_loss: 0.8905 - val_acc: 0.6771 - val_rmse_fer: 0.1140\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.67094 to 0.67707, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 67/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8696 - acc: 0.6735 - rmse_fer: 0.1129 - val_loss: 0.9114 - val_acc: 0.6581 - val_rmse_fer: 0.1167\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.67707\n",
      "Epoch 68/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8622 - acc: 0.6753 - rmse_fer: 0.1121 - val_loss: 0.9200 - val_acc: 0.6578 - val_rmse_fer: 0.1163\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.67707\n",
      "Epoch 69/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8612 - acc: 0.6771 - rmse_fer: 0.1123 - val_loss: 0.8784 - val_acc: 0.6751 - val_rmse_fer: 0.1137\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.67707\n",
      "Epoch 70/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8625 - acc: 0.6750 - rmse_fer: 0.1124 - val_loss: 0.9051 - val_acc: 0.6734 - val_rmse_fer: 0.1139\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.67707\n",
      "Epoch 71/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8586 - acc: 0.6765 - rmse_fer: 0.1122 - val_loss: 0.8908 - val_acc: 0.6718 - val_rmse_fer: 0.1139\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.67707\n",
      "Epoch 72/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.8482 - acc: 0.6807 - rmse_fer: 0.1118 - val_loss: 0.9314 - val_acc: 0.6548 - val_rmse_fer: 0.1170\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.67707\n",
      "Epoch 73/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8512 - acc: 0.6777 - rmse_fer: 0.1120 - val_loss: 0.9072 - val_acc: 0.6659 - val_rmse_fer: 0.1149\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.67707\n",
      "Epoch 74/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.8530 - acc: 0.6770 - rmse_fer: 0.1122 - val_loss: 0.9326 - val_acc: 0.6556 - val_rmse_fer: 0.1196\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.67707\n",
      "Epoch 75/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8449 - acc: 0.6810 - rmse_fer: 0.1117 - val_loss: 0.9032 - val_acc: 0.6604 - val_rmse_fer: 0.1156\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.67707\n",
      "Epoch 76/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8457 - acc: 0.6813 - rmse_fer: 0.1113 - val_loss: 0.8971 - val_acc: 0.6612 - val_rmse_fer: 0.1148\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.67707\n",
      "Epoch 77/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8380 - acc: 0.6844 - rmse_fer: 0.1110 - val_loss: 0.8839 - val_acc: 0.6757 - val_rmse_fer: 0.1130\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.67707\n",
      "Epoch 78/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8376 - acc: 0.6849 - rmse_fer: 0.1109 - val_loss: 0.8879 - val_acc: 0.6709 - val_rmse_fer: 0.1157\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.67707\n",
      "Epoch 79/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.8384 - acc: 0.6853 - rmse_fer: 0.1109 - val_loss: 0.8792 - val_acc: 0.6812 - val_rmse_fer: 0.1109\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.67707 to 0.68125, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 80/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8307 - acc: 0.6874 - rmse_fer: 0.1105 - val_loss: 0.8881 - val_acc: 0.6670 - val_rmse_fer: 0.1136\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.68125\n",
      "Epoch 81/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8262 - acc: 0.6896 - rmse_fer: 0.1102 - val_loss: 0.8917 - val_acc: 0.6762 - val_rmse_fer: 0.1166\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.68125\n",
      "Epoch 82/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8284 - acc: 0.6875 - rmse_fer: 0.1104 - val_loss: 0.8945 - val_acc: 0.6721 - val_rmse_fer: 0.1155\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.68125\n",
      "Epoch 83/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8246 - acc: 0.6896 - rmse_fer: 0.1102 - val_loss: 0.8938 - val_acc: 0.6773 - val_rmse_fer: 0.1108\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.68125\n",
      "Epoch 84/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.8238 - acc: 0.6899 - rmse_fer: 0.1101 - val_loss: 0.9064 - val_acc: 0.6715 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.68125\n",
      "Epoch 85/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8168 - acc: 0.6934 - rmse_fer: 0.1097 - val_loss: 0.9109 - val_acc: 0.6787 - val_rmse_fer: 0.1153\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.68125\n",
      "Epoch 86/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8164 - acc: 0.6924 - rmse_fer: 0.1096 - val_loss: 0.9218 - val_acc: 0.6670 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.68125\n",
      "Epoch 87/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8178 - acc: 0.6922 - rmse_fer: 0.1097 - val_loss: 0.9100 - val_acc: 0.6721 - val_rmse_fer: 0.1149\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.68125\n",
      "Epoch 88/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8133 - acc: 0.6942 - rmse_fer: 0.1094 - val_loss: 0.8942 - val_acc: 0.6732 - val_rmse_fer: 0.1146\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.68125\n",
      "Epoch 89/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8090 - acc: 0.6947 - rmse_fer: 0.1089 - val_loss: 0.9046 - val_acc: 0.6757 - val_rmse_fer: 0.1125\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.68125\n",
      "Epoch 90/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8070 - acc: 0.6947 - rmse_fer: 0.1089 - val_loss: 0.8800 - val_acc: 0.6810 - val_rmse_fer: 0.1124\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.68125\n",
      "Epoch 91/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8052 - acc: 0.6964 - rmse_fer: 0.1090 - val_loss: 0.9330 - val_acc: 0.6754 - val_rmse_fer: 0.1138\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.68125\n",
      "Epoch 92/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8067 - acc: 0.6969 - rmse_fer: 0.1085 - val_loss: 0.8993 - val_acc: 0.6773 - val_rmse_fer: 0.1130\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.68125\n",
      "Epoch 93/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8041 - acc: 0.6961 - rmse_fer: 0.1083 - val_loss: 0.9023 - val_acc: 0.6768 - val_rmse_fer: 0.1146\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.68125\n",
      "Epoch 94/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.8013 - acc: 0.6983 - rmse_fer: 0.1085 - val_loss: 0.9308 - val_acc: 0.6698 - val_rmse_fer: 0.1185\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.68125\n",
      "Epoch 95/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7967 - acc: 0.6995 - rmse_fer: 0.1081 - val_loss: 0.9266 - val_acc: 0.6612 - val_rmse_fer: 0.1148\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.68125\n",
      "Epoch 96/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7932 - acc: 0.7023 - rmse_fer: 0.1074 - val_loss: 0.8973 - val_acc: 0.6812 - val_rmse_fer: 0.1136\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.68125\n",
      "Epoch 97/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7909 - acc: 0.7021 - rmse_fer: 0.1071 - val_loss: 0.9248 - val_acc: 0.6654 - val_rmse_fer: 0.1155\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.68125\n",
      "Epoch 98/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7887 - acc: 0.7036 - rmse_fer: 0.1075 - val_loss: 0.9023 - val_acc: 0.6695 - val_rmse_fer: 0.1138\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.68125\n",
      "Epoch 99/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7892 - acc: 0.7011 - rmse_fer: 0.1074 - val_loss: 0.9116 - val_acc: 0.6785 - val_rmse_fer: 0.1133\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.68125\n",
      "Epoch 100/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7834 - acc: 0.7039 - rmse_fer: 0.1067 - val_loss: 0.9262 - val_acc: 0.6651 - val_rmse_fer: 0.1156\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.68125\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7847 - acc: 0.7030 - rmse_fer: 0.1070 - val_loss: 0.9132 - val_acc: 0.6682 - val_rmse_fer: 0.1131\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.68125\n",
      "Epoch 102/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7828 - acc: 0.7044 - rmse_fer: 0.1069 - val_loss: 0.9178 - val_acc: 0.6773 - val_rmse_fer: 0.1132\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.68125\n",
      "Epoch 103/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7766 - acc: 0.7067 - rmse_fer: 0.1064 - val_loss: 0.9215 - val_acc: 0.6785 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.68125\n",
      "Epoch 104/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7734 - acc: 0.7088 - rmse_fer: 0.1058 - val_loss: 0.9187 - val_acc: 0.6704 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.68125\n",
      "Epoch 105/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7758 - acc: 0.7090 - rmse_fer: 0.1062 - val_loss: 0.9095 - val_acc: 0.6762 - val_rmse_fer: 0.1141\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.68125\n",
      "Epoch 106/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7678 - acc: 0.7103 - rmse_fer: 0.1053 - val_loss: 0.8914 - val_acc: 0.6771 - val_rmse_fer: 0.1110\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.68125\n",
      "Epoch 107/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7706 - acc: 0.7091 - rmse_fer: 0.1057 - val_loss: 0.9329 - val_acc: 0.6712 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.68125\n",
      "Epoch 108/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7714 - acc: 0.7086 - rmse_fer: 0.1060 - val_loss: 0.9051 - val_acc: 0.6732 - val_rmse_fer: 0.1117\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.68125\n",
      "Epoch 109/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7665 - acc: 0.7119 - rmse_fer: 0.1054 - val_loss: 0.9257 - val_acc: 0.6760 - val_rmse_fer: 0.1137\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.68125\n",
      "Epoch 110/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7625 - acc: 0.7136 - rmse_fer: 0.1054 - val_loss: 0.9712 - val_acc: 0.6592 - val_rmse_fer: 0.1196\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.68125\n",
      "Epoch 111/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7642 - acc: 0.7115 - rmse_fer: 0.1053 - val_loss: 0.9156 - val_acc: 0.6679 - val_rmse_fer: 0.1154\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.68125\n",
      "Epoch 112/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7605 - acc: 0.7125 - rmse_fer: 0.1051 - val_loss: 0.9199 - val_acc: 0.6771 - val_rmse_fer: 0.1160\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.68125\n",
      "Epoch 113/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7549 - acc: 0.7154 - rmse_fer: 0.1046 - val_loss: 0.9120 - val_acc: 0.6793 - val_rmse_fer: 0.1141\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.68125\n",
      "Epoch 114/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7547 - acc: 0.7166 - rmse_fer: 0.1048 - val_loss: 0.9018 - val_acc: 0.6860 - val_rmse_fer: 0.1128\n",
      "\n",
      "Epoch 00114: val_acc improved from 0.68125 to 0.68598, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 115/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7524 - acc: 0.7161 - rmse_fer: 0.1047 - val_loss: 0.9401 - val_acc: 0.6693 - val_rmse_fer: 0.1155\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.68598\n",
      "Epoch 116/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7511 - acc: 0.7167 - rmse_fer: 0.1046 - val_loss: 0.9069 - val_acc: 0.6824 - val_rmse_fer: 0.1117\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.68598\n",
      "Epoch 117/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7478 - acc: 0.7176 - rmse_fer: 0.1040 - val_loss: 0.9282 - val_acc: 0.6698 - val_rmse_fer: 0.1154\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.68598\n",
      "Epoch 118/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7482 - acc: 0.7187 - rmse_fer: 0.1040 - val_loss: 0.9095 - val_acc: 0.6865 - val_rmse_fer: 0.1127\n",
      "\n",
      "Epoch 00118: val_acc improved from 0.68598 to 0.68654, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 119/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7452 - acc: 0.7189 - rmse_fer: 0.1037 - val_loss: 0.9213 - val_acc: 0.6807 - val_rmse_fer: 0.1138\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.68654\n",
      "Epoch 120/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7464 - acc: 0.7188 - rmse_fer: 0.1040 - val_loss: 0.9277 - val_acc: 0.6807 - val_rmse_fer: 0.1137\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.68654\n",
      "Epoch 121/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7406 - acc: 0.7209 - rmse_fer: 0.1035 - val_loss: 0.9200 - val_acc: 0.6779 - val_rmse_fer: 0.1130\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.68654\n",
      "Epoch 122/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7374 - acc: 0.7221 - rmse_fer: 0.1033 - val_loss: 0.9541 - val_acc: 0.6729 - val_rmse_fer: 0.1136\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.68654\n",
      "Epoch 123/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7407 - acc: 0.7218 - rmse_fer: 0.1032 - val_loss: 0.9238 - val_acc: 0.6793 - val_rmse_fer: 0.1127\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.68654\n",
      "Epoch 124/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7390 - acc: 0.7209 - rmse_fer: 0.1031 - val_loss: 0.9234 - val_acc: 0.6812 - val_rmse_fer: 0.1132\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.68654\n",
      "Epoch 125/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7341 - acc: 0.7228 - rmse_fer: 0.1028 - val_loss: 0.9158 - val_acc: 0.6849 - val_rmse_fer: 0.1118\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.68654\n",
      "Epoch 126/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7278 - acc: 0.7244 - rmse_fer: 0.1024 - val_loss: 0.9273 - val_acc: 0.6715 - val_rmse_fer: 0.1149\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.68654\n",
      "Epoch 127/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7263 - acc: 0.7258 - rmse_fer: 0.1025 - val_loss: 0.9424 - val_acc: 0.6799 - val_rmse_fer: 0.1123\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.68654\n",
      "Epoch 128/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7275 - acc: 0.7251 - rmse_fer: 0.1024 - val_loss: 0.9088 - val_acc: 0.6863 - val_rmse_fer: 0.1122\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.68654\n",
      "Epoch 129/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7214 - acc: 0.7262 - rmse_fer: 0.1019 - val_loss: 0.9448 - val_acc: 0.6801 - val_rmse_fer: 0.1134\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.68654\n",
      "Epoch 130/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7216 - acc: 0.7271 - rmse_fer: 0.1018 - val_loss: 0.9423 - val_acc: 0.6773 - val_rmse_fer: 0.1136\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.68654\n",
      "Epoch 131/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7178 - acc: 0.7272 - rmse_fer: 0.1015 - val_loss: 0.9433 - val_acc: 0.6810 - val_rmse_fer: 0.1118\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.68654\n",
      "Epoch 132/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7129 - acc: 0.7316 - rmse_fer: 0.1011 - val_loss: 0.9591 - val_acc: 0.6659 - val_rmse_fer: 0.1147\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.68654\n",
      "Epoch 133/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7213 - acc: 0.7275 - rmse_fer: 0.1017 - val_loss: 0.9605 - val_acc: 0.6718 - val_rmse_fer: 0.1141\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.68654\n",
      "Epoch 134/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7163 - acc: 0.7304 - rmse_fer: 0.1013 - val_loss: 0.9355 - val_acc: 0.6765 - val_rmse_fer: 0.1123\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.68654\n",
      "Epoch 135/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7114 - acc: 0.7320 - rmse_fer: 0.1010 - val_loss: 0.9415 - val_acc: 0.6751 - val_rmse_fer: 0.1132\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.68654\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7096 - acc: 0.7331 - rmse_fer: 0.1008 - val_loss: 0.9244 - val_acc: 0.6871 - val_rmse_fer: 0.1105\n",
      "\n",
      "Epoch 00136: val_acc improved from 0.68654 to 0.68710, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 137/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7102 - acc: 0.7316 - rmse_fer: 0.1007 - val_loss: 0.9249 - val_acc: 0.6846 - val_rmse_fer: 0.1123\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.68710\n",
      "Epoch 138/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7054 - acc: 0.7345 - rmse_fer: 0.1004 - val_loss: 0.9420 - val_acc: 0.6771 - val_rmse_fer: 0.1131\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.68710\n",
      "Epoch 139/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7063 - acc: 0.7338 - rmse_fer: 0.1008 - val_loss: 0.9452 - val_acc: 0.6812 - val_rmse_fer: 0.1136\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.68710\n",
      "Epoch 140/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7003 - acc: 0.7370 - rmse_fer: 0.0999 - val_loss: 0.9144 - val_acc: 0.6849 - val_rmse_fer: 0.1110\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.68710\n",
      "Epoch 141/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.7005 - acc: 0.7355 - rmse_fer: 0.1000 - val_loss: 0.9347 - val_acc: 0.6824 - val_rmse_fer: 0.1127\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.68710\n",
      "Epoch 142/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.7001 - acc: 0.7358 - rmse_fer: 0.1000 - val_loss: 0.9071 - val_acc: 0.6874 - val_rmse_fer: 0.1095\n",
      "\n",
      "Epoch 00142: val_acc improved from 0.68710 to 0.68738, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 143/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6969 - acc: 0.7358 - rmse_fer: 0.0996 - val_loss: 0.9228 - val_acc: 0.6860 - val_rmse_fer: 0.1111\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.68738\n",
      "Epoch 144/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6981 - acc: 0.7354 - rmse_fer: 0.0996 - val_loss: 0.9225 - val_acc: 0.6835 - val_rmse_fer: 0.1123\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.68738\n",
      "Epoch 145/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6910 - acc: 0.7392 - rmse_fer: 0.0991 - val_loss: 0.9863 - val_acc: 0.6712 - val_rmse_fer: 0.1154\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.68738\n",
      "Epoch 146/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6866 - acc: 0.7413 - rmse_fer: 0.0988 - val_loss: 0.9739 - val_acc: 0.6709 - val_rmse_fer: 0.1163\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.68738\n",
      "Epoch 147/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6877 - acc: 0.7402 - rmse_fer: 0.0990 - val_loss: 1.0053 - val_acc: 0.6682 - val_rmse_fer: 0.1151\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.68738\n",
      "Epoch 148/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6895 - acc: 0.7402 - rmse_fer: 0.0989 - val_loss: 0.9467 - val_acc: 0.6824 - val_rmse_fer: 0.1130\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.68738\n",
      "Epoch 149/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6830 - acc: 0.7416 - rmse_fer: 0.0988 - val_loss: 0.9884 - val_acc: 0.6801 - val_rmse_fer: 0.1150\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.68738\n",
      "Epoch 150/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6852 - acc: 0.7413 - rmse_fer: 0.0987 - val_loss: 1.0129 - val_acc: 0.6648 - val_rmse_fer: 0.1179\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.68738\n",
      "Epoch 151/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6792 - acc: 0.7432 - rmse_fer: 0.0982 - val_loss: 0.9306 - val_acc: 0.6885 - val_rmse_fer: 0.1111\n",
      "\n",
      "Epoch 00151: val_acc improved from 0.68738 to 0.68849, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 152/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6783 - acc: 0.7424 - rmse_fer: 0.0979 - val_loss: 0.9850 - val_acc: 0.6762 - val_rmse_fer: 0.1139\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.68849\n",
      "Epoch 153/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6812 - acc: 0.7437 - rmse_fer: 0.0982 - val_loss: 0.9816 - val_acc: 0.6751 - val_rmse_fer: 0.1144\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.68849\n",
      "Epoch 154/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6751 - acc: 0.7450 - rmse_fer: 0.0978 - val_loss: 0.9759 - val_acc: 0.6796 - val_rmse_fer: 0.1133\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.68849\n",
      "Epoch 155/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6742 - acc: 0.7454 - rmse_fer: 0.0977 - val_loss: 0.9762 - val_acc: 0.6751 - val_rmse_fer: 0.1145\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.68849\n",
      "Epoch 156/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6698 - acc: 0.7451 - rmse_fer: 0.0976 - val_loss: 0.9972 - val_acc: 0.6704 - val_rmse_fer: 0.1163\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.68849\n",
      "Epoch 157/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6694 - acc: 0.7467 - rmse_fer: 0.0976 - val_loss: 0.9381 - val_acc: 0.6935 - val_rmse_fer: 0.1103\n",
      "\n",
      "Epoch 00157: val_acc improved from 0.68849 to 0.69351, saving model to Final_Subm_FER_Tony_v1_.best.hdf5\n",
      "Epoch 158/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6678 - acc: 0.7468 - rmse_fer: 0.0974 - val_loss: 0.9799 - val_acc: 0.6924 - val_rmse_fer: 0.1126\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.69351\n",
      "Epoch 159/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6676 - acc: 0.7496 - rmse_fer: 0.0970 - val_loss: 0.9779 - val_acc: 0.6826 - val_rmse_fer: 0.1120\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.69351\n",
      "Epoch 160/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6629 - acc: 0.7507 - rmse_fer: 0.0965 - val_loss: 0.9775 - val_acc: 0.6885 - val_rmse_fer: 0.1137\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.69351\n",
      "Epoch 161/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6652 - acc: 0.7474 - rmse_fer: 0.0969 - val_loss: 0.9645 - val_acc: 0.6849 - val_rmse_fer: 0.1120\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.69351\n",
      "Epoch 162/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6631 - acc: 0.7497 - rmse_fer: 0.0967 - val_loss: 0.9786 - val_acc: 0.6857 - val_rmse_fer: 0.1113\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.69351\n",
      "Epoch 163/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6582 - acc: 0.7527 - rmse_fer: 0.0960 - val_loss: 0.9551 - val_acc: 0.6918 - val_rmse_fer: 0.1105\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.69351\n",
      "Epoch 164/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6578 - acc: 0.7522 - rmse_fer: 0.0960 - val_loss: 0.9859 - val_acc: 0.6815 - val_rmse_fer: 0.1140\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.69351\n",
      "Epoch 165/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6578 - acc: 0.7513 - rmse_fer: 0.0961 - val_loss: 0.9773 - val_acc: 0.6799 - val_rmse_fer: 0.1125\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.69351\n",
      "Epoch 166/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6507 - acc: 0.7534 - rmse_fer: 0.0958 - val_loss: 0.9629 - val_acc: 0.6893 - val_rmse_fer: 0.1114\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.69351\n",
      "Epoch 167/200\n",
      "503/503 [==============================] - 79s 156ms/step - loss: 0.6514 - acc: 0.7534 - rmse_fer: 0.0957 - val_loss: 0.9805 - val_acc: 0.6930 - val_rmse_fer: 0.1095\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.69351\n",
      "Epoch 168/200\n",
      "503/503 [==============================] - 79s 157ms/step - loss: 0.6439 - acc: 0.7574 - rmse_fer: 0.0948 - val_loss: 0.9825 - val_acc: 0.6871 - val_rmse_fer: 0.1119\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.69351\n",
      "Epoch 169/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6487 - acc: 0.7555 - rmse_fer: 0.0954 - val_loss: 1.0002 - val_acc: 0.6821 - val_rmse_fer: 0.1134\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.69351\n",
      "Epoch 170/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6498 - acc: 0.7559 - rmse_fer: 0.0954 - val_loss: 0.9790 - val_acc: 0.6746 - val_rmse_fer: 0.1134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00170: val_acc did not improve from 0.69351\n",
      "Epoch 171/200\n",
      "503/503 [==============================] - 78s 156ms/step - loss: 0.6473 - acc: 0.7567 - rmse_fer: 0.0953 - val_loss: 0.9447 - val_acc: 0.6877 - val_rmse_fer: 0.1101\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.69351\n",
      "Epoch 172/200\n",
      "438/503 [=========================>....] - ETA: 9s - loss: 0.6359 - acc: 0.7591 - rmse_fer: 0.0945 "
     ]
    }
   ],
   "source": [
    "model_cnn.fit_generator(datagen.flow(X_train, Y_train,\n",
    "          batch_size=batch_size),\n",
    "          epochs=nb_epcohs,\n",
    "          verbose=1,\n",
    "          class_weight=class_weights,\n",
    "          validation_data=(X_val, Y_val),\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model's performance on Private Leader Board data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 1 Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "filepath = \"Final_Subm_FER_Tony_v1_\" + r\".best.hdf5\"\n",
    "model = load_model(filepath, custom_objects={'rmse_fer': rmse_fer})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Verfiy the performace on train and validation data (just for confromation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32181/32181 [==============================] - 17s 533us/step\n",
      "Training loss: 0.41371478231156733\n",
      "Training accuracy: 0.86796556975168\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Training loss:', score[0])\n",
    "print('Training accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589/3589 [==============================] - 2s 516us/step\n",
      "Public Leader Board Test loss: 1.182809711532295\n",
      "Public Leader Board Test accuracy: 0.6960156032487057\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_val, Y_val, verbose=1)\n",
    "print('Public Leader Board Test loss:', score[0])\n",
    "print('Public Leader Board Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Test on Private Leader Board data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589/3589 [==============================] - 2s 505us/step\n",
      "Private Leader Board Test loss: 1.182809711532295\n",
      "Private Leader Board Test accuracy: 0.6960156032487057\n"
     ]
    }
   ],
   "source": [
    "#score = new_model.evaluate(X_private_test, Y_private_test, verbose=1)\n",
    "score = model.evaluate(X_private_test, Y_private_test, verbose=1)\n",
    "print('Private Leader Board Test loss:', score[0])\n",
    "print('Private Leader Board Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  Analyse the results of the stand alone model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1  plot confusion matrix - check how the model performed on various classes (Training and Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict_proba(X_train)\n",
    "Y_val_pred   = model.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train = []\n",
    "y_pred_train = []\n",
    "\n",
    "for row in Y_train:\n",
    "    a = numpy.argmax(row) #return the indices of the maximum values along the axis\n",
    "    y_true_train.append(a)\n",
    "    \n",
    "for row in Y_train_pred:\n",
    "    a = numpy.argmax(row) #return tY_train_predhe indices of the maximum values along the axis\n",
    "    y_pred_train.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_val = []\n",
    "y_pred_val = []\n",
    "\n",
    "for row in Y_val:\n",
    "    a = numpy.argmax(row) #return the indices of the maximum values along the axis\n",
    "    y_true_val.append(a)\n",
    "    \n",
    "for row in Y_val_pred:\n",
    "    a = numpy.argmax(row) #return tY_train_predhe indices of the maximum values along the axis\n",
    "    y_pred_val.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_train     = confusion_matrix(y_true_train, y_pred_train)\n",
    "cnf_matrix_val       = confusion_matrix(y_true_val, y_pred_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_train, classes=Facial_Expressions, normalize=True,\n",
    "                      title='Normalized confusion matrix for Train Data')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_val, classes=Facial_Expressions, normalize=True,\n",
    "                      title='Normalized confusion matrix for Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2  Observations\n",
    "\n",
    "- Here we can see on what classes the model performed well and bad\n",
    "- Fear and Neutral classes are performing badly on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Build Different ensemble models to further  increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Simple Average Ensembling (using Randomly perturbed input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Testing data again\n",
    "predict_prob to work we need input images in 48x48 format. So we load them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "private_set_x, private_set_y = HelperDataProcessing.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba0 = predict_prob(0, private_set_x, model)\n",
    "proba1 = predict_prob(1, private_set_x, model)\n",
    "proba2 = predict_prob(2, private_set_x, model)\n",
    "proba3 = predict_prob(3, private_set_x, model)\n",
    "proba4 = predict_prob(4, private_set_x, model)\n",
    "proba5 = predict_prob(5, private_set_x, model)\n",
    "proba6 = predict_prob(6, private_set_x, model)\n",
    "proba7 = predict_prob(7, private_set_x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_SA_Ense_pred = []\n",
    "for row in zip(proba0,proba1,proba2,proba3,proba4,proba5,proba6,proba7):\n",
    "    a = numpy.argmax(np.array(row).mean(axis=0)) #return the indices of the maximum values along the axis\n",
    "    Y_SA_Ense_pred.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_SA_Ense_pred = np.array(Y_SA_Ense_pred)\n",
    "private_set_y  = np.array(private_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate how many correct predictions we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = np.sum(Y_SA_Ense_pred == private_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Simple Average Ensembling Method on Private Leader Board Data:0.7035385901365283\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Simple Average Ensembling Method on Private Leader Board Data:\"+str((float(count_correct)/len(Y_SA_Ense_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix_SA_ense     = confusion_matrix(Y_SA_Ense_pred, private_set_y)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_SA_ense, classes=Facial_Expressions, normalize=True,\n",
    "                      title='Normalized confusion matrix for Test Data - Using Simple Average Ensembling ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Simple Average Ensembling - Observations\n",
    "\n",
    "- Using Simple Average Ensembling, we could  slightly improve the performace of the prediction model\n",
    "    - from **0.6960** to **0.7035**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Optimal Hinge Loss Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the train test and validation data for ensembling NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training and Validation data again\n",
    "predict_prob to work we need input images in 48x48 format. So we load training and validation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val  = HelperDataProcessing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Non-string object detected for the array ordering. Please pass in 'C', 'F', 'A', or 'K' instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = HelperDataProcessing.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 32181 <class 'list'> 3589 <class 'list'> 3589\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), len(X_train), type(X_val), len(X_val), type(X_test), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = [np.array(x).reshape(48,48) for x in X_train]\n",
    "Y_train_t = np_utils.to_categorical(y_train, 7)\n",
    "X_val_t   = [np.array(x).reshape(48,48) for x in X_val]\n",
    "Y_val_t   = np_utils.to_categorical(y_val, 7)\n",
    "Y_test_t   = np_utils.to_categorical(y_test, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 32181, numpy.ndarray, 32181)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_t), len(X_train_t),type(Y_train_t), len(Y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 3589, numpy.ndarray, 3589)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_val_t), len(X_val_t),type(Y_val_t), len(Y_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_val_t), len(X_val_t),type(Y_val_t), len(Y_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba0 = predict_prob(0, X_train_t, model)\n",
    "proba1 = predict_prob(1, X_train_t, model)\n",
    "proba2 = predict_prob(2, X_train_t, model)\n",
    "proba3 = predict_prob(3, X_train_t, model)\n",
    "proba4 = predict_prob(4, X_train_t, model)\n",
    "proba5 = predict_prob(5, X_train_t, model)\n",
    "proba6 = predict_prob(6, X_train_t, model)\n",
    "proba7 = predict_prob(7, X_train_t, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proba0_val = predict_prob(0, X_val_t, model)\n",
    "proba1_val = predict_prob(1, X_val_t, model)\n",
    "proba2_val = predict_prob(2, X_val_t, model)\n",
    "proba3_val = predict_prob(3, X_val_t, model)\n",
    "proba4_val = predict_prob(4, X_val_t, model)\n",
    "proba5_val = predict_prob(5, X_val_t, model)\n",
    "proba6_val = predict_prob(6, X_val_t, model)\n",
    "proba7_val = predict_prob(7, X_val_t, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba0_y = predict_prob(0, private_set_x, model)\n",
    "proba1_y = predict_prob(1, private_set_x, model)\n",
    "proba2_y = predict_prob(2, private_set_x, model)\n",
    "proba3_y = predict_prob(3, private_set_x, model)\n",
    "proba4_y = predict_prob(4, private_set_x, model)\n",
    "proba5_y = predict_prob(5, private_set_x, model)\n",
    "proba6_y = predict_prob(6, private_set_x, model)\n",
    "proba7_y = predict_prob(7, private_set_x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ensemble = []\n",
    "for row in zip(proba0,proba1,proba2,proba3,proba4,proba5,proba6,proba7):\n",
    "    x_train_ensemble.append(row)\n",
    "    \n",
    "x_val_ensemble = []\n",
    "for row in zip(proba0_val,proba1_val,proba2_val,proba3_val,proba4_val,proba5_val,proba6_val,proba7_val):\n",
    "    x_val_ensemble.append(row)\n",
    "    \n",
    "x_test_ensemble = []\n",
    "for row in zip(proba0_y,proba1_y,proba2_y,proba3_y,proba4_y,proba5_y,proba6_y,proba7_y):\n",
    "    x_test_ensemble.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ensemble = np.array(x_train_ensemble)\n",
    "x_val_ensemble   = np.array(x_val_ensemble)\n",
    "x_test_ensemble  = np.array(x_test_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ensemble = x_train_ensemble.reshape(32181, 56, 1)\n",
    "x_val_ensemble   = x_val_ensemble.reshape(3589, 56, 1)\n",
    "x_test_ensemble  = x_test_ensemble.reshape(3589, 56, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32181, 56, 1),\n",
       " (32181, 7),\n",
       " (3589, 56, 1),\n",
       " (3589, 7),\n",
       " (3589, 56, 1),\n",
       " (3589, 7))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_ensemble.shape, Y_train_t.shape, x_val_ensemble.shape, Y_val_t.shape, x_test_ensemble.shape, Y_test_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define all the callback functions for the ensembling NN  model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback for saving the best ensembling NN model\n",
    "\n",
    "filepath = file_prefix + r\"ensemble_hinge.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the ensembling NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model_generate():\n",
    "    nb_classes, nb_peturbations = 7, 8\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(nb_classes* nb_peturbations, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #categorical_hinge, squared_hinge, mean_squared_error,sparse_categorical_crossentropy\n",
    "    model.compile(loss='mean_squared_error',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1024)              58368     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 7)                 3591      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,636,359\n",
      "Trainable params: 1,636,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del ensemble_model # does model exist in the current namespace\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "ensemble_model = ensemble_model_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the ensembling NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32181 samples, validate on 3589 samples\n",
      "Epoch 1/120\n",
      "32181/32181 [==============================] - 2s 59us/step - loss: 0.0973 - acc: 0.7200 - val_loss: 0.0790 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67818, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 2/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0447 - acc: 0.8635 - val_loss: 0.0607 - val_acc: 0.6974\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67818 to 0.69741, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 3/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0272 - acc: 0.8771 - val_loss: 0.0618 - val_acc: 0.6982\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.69741 to 0.69824, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 4/120\n",
      "32181/32181 [==============================] - 1s 43us/step - loss: 0.0251 - acc: 0.8794 - val_loss: 0.0627 - val_acc: 0.6974\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69824\n",
      "Epoch 5/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0245 - acc: 0.8805 - val_loss: 0.0632 - val_acc: 0.6980\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69824\n",
      "Epoch 6/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0240 - acc: 0.8821 - val_loss: 0.0633 - val_acc: 0.7044\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.69824 to 0.70437, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 7/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0236 - acc: 0.8913 - val_loss: 0.0634 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.70437 to 0.70716, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 8/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0232 - acc: 0.8943 - val_loss: 0.0632 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.70716 to 0.70911, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 9/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0227 - acc: 0.8956 - val_loss: 0.0631 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.70911 to 0.70939, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 10/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0223 - acc: 0.8965 - val_loss: 0.0630 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.70939 to 0.70967, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 11/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0220 - acc: 0.8981 - val_loss: 0.0630 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.70967\n",
      "Epoch 12/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0218 - acc: 0.8986 - val_loss: 0.0630 - val_acc: 0.7069\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.70967\n",
      "Epoch 13/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0216 - acc: 0.8997 - val_loss: 0.0630 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.70967\n",
      "Epoch 14/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0214 - acc: 0.8997 - val_loss: 0.0630 - val_acc: 0.7077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.70967\n",
      "Epoch 15/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0213 - acc: 0.9004 - val_loss: 0.0630 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70967\n",
      "Epoch 16/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0212 - acc: 0.9005 - val_loss: 0.0630 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70967\n",
      "Epoch 17/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0211 - acc: 0.9013 - val_loss: 0.0629 - val_acc: 0.7077\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70967\n",
      "Epoch 18/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0210 - acc: 0.9014 - val_loss: 0.0630 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.70967\n",
      "Epoch 19/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0210 - acc: 0.9016 - val_loss: 0.0628 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.70967\n",
      "Epoch 20/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0209 - acc: 0.9018 - val_loss: 0.0629 - val_acc: 0.7077\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.70967\n",
      "Epoch 21/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0208 - acc: 0.9022 - val_loss: 0.0629 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.70967\n",
      "Epoch 22/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0208 - acc: 0.9021 - val_loss: 0.0629 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.70967\n",
      "Epoch 23/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0207 - acc: 0.9027 - val_loss: 0.0629 - val_acc: 0.7069\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.70967\n",
      "Epoch 24/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0207 - acc: 0.9028 - val_loss: 0.0628 - val_acc: 0.7069\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.70967\n",
      "Epoch 25/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0206 - acc: 0.9029 - val_loss: 0.0629 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.70967\n",
      "Epoch 26/120\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0206 - acc: 0.9026 - val_loss: 0.0628 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.70967\n",
      "Epoch 27/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0206 - acc: 0.9030 - val_loss: 0.0629 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.70967\n",
      "Epoch 28/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0205 - acc: 0.9031 - val_loss: 0.0630 - val_acc: 0.7074\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.70967\n",
      "Epoch 29/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0205 - acc: 0.9030 - val_loss: 0.0629 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.70967\n",
      "Epoch 30/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0205 - acc: 0.9037 - val_loss: 0.0629 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.70967\n",
      "Epoch 31/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0204 - acc: 0.9033 - val_loss: 0.0630 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.70967\n",
      "Epoch 32/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0204 - acc: 0.9033 - val_loss: 0.0630 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.70967\n",
      "Epoch 33/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0204 - acc: 0.9036 - val_loss: 0.0630 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.70967\n",
      "Epoch 34/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0204 - acc: 0.9035 - val_loss: 0.0630 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.70967\n",
      "Epoch 35/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0203 - acc: 0.9038 - val_loss: 0.0630 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.70967 to 0.70995, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 36/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0203 - acc: 0.9039 - val_loss: 0.0630 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.70995\n",
      "Epoch 37/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0203 - acc: 0.9041 - val_loss: 0.0630 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.70995\n",
      "Epoch 38/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0203 - acc: 0.9042 - val_loss: 0.0630 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.70995 to 0.71023, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 39/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0203 - acc: 0.9044 - val_loss: 0.0631 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.71023 to 0.71078, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 40/120\n",
      "32181/32181 [==============================] - 1s 43us/step - loss: 0.0203 - acc: 0.9044 - val_loss: 0.0631 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.71078\n",
      "Epoch 41/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9043 - val_loss: 0.0631 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.71078\n",
      "Epoch 42/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9044 - val_loss: 0.0631 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.71078 to 0.71190, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 43/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9045 - val_loss: 0.0631 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.71190\n",
      "Epoch 44/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0202 - acc: 0.9048 - val_loss: 0.0631 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.71190 to 0.71218, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 45/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9049 - val_loss: 0.0630 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.71218\n",
      "Epoch 46/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9042 - val_loss: 0.0631 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.71218 to 0.71273, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 47/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9044 - val_loss: 0.0631 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.71273\n",
      "Epoch 48/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9044 - val_loss: 0.0631 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.71273\n",
      "Epoch 49/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9049 - val_loss: 0.0631 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.71273\n",
      "Epoch 50/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9049 - val_loss: 0.0632 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.71273\n",
      "Epoch 51/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9052 - val_loss: 0.0631 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.71273\n",
      "Epoch 52/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0201 - acc: 0.9053 - val_loss: 0.0631 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.71273\n",
      "Epoch 53/120\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0201 - acc: 0.9050 - val_loss: 0.0632 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.71273\n",
      "Epoch 54/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9051 - val_loss: 0.0632 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.71273\n",
      "Epoch 55/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9053 - val_loss: 0.0632 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.71273 to 0.71301, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 56/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9054 - val_loss: 0.0632 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.71301\n",
      "Epoch 57/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9051 - val_loss: 0.0632 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.71301\n",
      "Epoch 58/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9053 - val_loss: 0.0632 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.71301\n",
      "Epoch 59/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9051 - val_loss: 0.0632 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.71301 to 0.71329, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 60/120\n",
      "32181/32181 [==============================] - 1s 43us/step - loss: 0.0200 - acc: 0.9050 - val_loss: 0.0632 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.71329\n",
      "Epoch 61/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9053 - val_loss: 0.0632 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.71329\n",
      "Epoch 62/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9059 - val_loss: 0.0633 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.71329\n",
      "Epoch 63/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9053 - val_loss: 0.0633 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.71329 to 0.71357, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 64/120\n",
      "32181/32181 [==============================] - 1s 43us/step - loss: 0.0200 - acc: 0.9058 - val_loss: 0.0632 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.71357\n",
      "Epoch 65/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9054 - val_loss: 0.0632 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.71357\n",
      "Epoch 66/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9062 - val_loss: 0.0632 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.71357\n",
      "Epoch 67/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9059 - val_loss: 0.0633 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.71357\n",
      "Epoch 68/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9057 - val_loss: 0.0632 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.71357\n",
      "Epoch 69/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9056 - val_loss: 0.0633 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.71357\n",
      "Epoch 70/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9058 - val_loss: 0.0633 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.71357\n",
      "Epoch 71/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9059 - val_loss: 0.0632 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.71357\n",
      "Epoch 72/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9060 - val_loss: 0.0633 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.71357\n",
      "Epoch 73/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9058 - val_loss: 0.0633 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.71357\n",
      "Epoch 74/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9057 - val_loss: 0.0632 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.71357\n",
      "Epoch 75/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9064 - val_loss: 0.0632 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.71357\n",
      "Epoch 76/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9063 - val_loss: 0.0633 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.71357\n",
      "Epoch 77/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9063 - val_loss: 0.0632 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.71357\n",
      "Epoch 78/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9066 - val_loss: 0.0633 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.71357\n",
      "Epoch 79/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0199 - acc: 0.9061 - val_loss: 0.0633 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.71357\n",
      "Epoch 80/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9067 - val_loss: 0.0633 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.71357\n",
      "Epoch 81/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9066 - val_loss: 0.0633 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.71357\n",
      "Epoch 82/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9067 - val_loss: 0.0633 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.71357\n",
      "Epoch 83/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9065 - val_loss: 0.0633 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.71357\n",
      "Epoch 84/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9067 - val_loss: 0.0633 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.71357 to 0.71468, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble.best.hdf5\n",
      "Epoch 85/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9063 - val_loss: 0.0633 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.71468\n",
      "Epoch 86/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9064 - val_loss: 0.0633 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.71468\n",
      "Epoch 87/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9065 - val_loss: 0.0634 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.71468\n",
      "Epoch 88/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9066 - val_loss: 0.0633 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.71468\n",
      "Epoch 89/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9068 - val_loss: 0.0633 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.71468\n",
      "Epoch 90/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9067 - val_loss: 0.0634 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.71468\n",
      "Epoch 91/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9066 - val_loss: 0.0634 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.71468\n",
      "Epoch 92/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9064 - val_loss: 0.0633 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.71468\n",
      "Epoch 93/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9069 - val_loss: 0.0633 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.71468\n",
      "Epoch 94/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9068 - val_loss: 0.0633 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.71468\n",
      "Epoch 95/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9067 - val_loss: 0.0633 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.71468\n",
      "Epoch 96/120\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0198 - acc: 0.9071 - val_loss: 0.0634 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.71468\n",
      "Epoch 97/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9074 - val_loss: 0.0633 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.71468\n",
      "Epoch 98/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9069 - val_loss: 0.0633 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.71468\n",
      "Epoch 99/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9068 - val_loss: 0.0634 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.71468\n",
      "Epoch 100/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9071 - val_loss: 0.0634 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.71468\n",
      "Epoch 101/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9072 - val_loss: 0.0634 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.71468\n",
      "Epoch 102/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9070 - val_loss: 0.0634 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.71468\n",
      "Epoch 103/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9073 - val_loss: 0.0634 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.71468\n",
      "Epoch 104/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9075 - val_loss: 0.0634 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.71468\n",
      "Epoch 105/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9072 - val_loss: 0.0635 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.71468\n",
      "Epoch 106/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9075 - val_loss: 0.0635 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.71468\n",
      "Epoch 107/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9067 - val_loss: 0.0634 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.71468\n",
      "Epoch 108/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9079 - val_loss: 0.0634 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.71468\n",
      "Epoch 109/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9073 - val_loss: 0.0634 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.71468\n",
      "Epoch 110/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9074 - val_loss: 0.0635 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.71468\n",
      "Epoch 111/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9072 - val_loss: 0.0634 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.71468\n",
      "Epoch 112/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9072 - val_loss: 0.0634 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.71468\n",
      "Epoch 113/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9073 - val_loss: 0.0634 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.71468\n",
      "Epoch 114/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9079 - val_loss: 0.0634 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.71468\n",
      "Epoch 115/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9077 - val_loss: 0.0634 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.71468\n",
      "Epoch 116/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9075 - val_loss: 0.0634 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.71468\n",
      "Epoch 117/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9070 - val_loss: 0.0634 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.71468\n",
      "Epoch 118/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9077 - val_loss: 0.0634 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.71468\n",
      "Epoch 119/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9076 - val_loss: 0.0634 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.71468\n",
      "Epoch 120/120\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9077 - val_loss: 0.0634 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.71468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbeaa0b4f98>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model.fit(x_train_ensemble, Y_train_t,\n",
    "          batch_size=128,\n",
    "          epochs=120,\n",
    "          verbose=1,\n",
    "          class_weight=class_weights,\n",
    "          validation_data=(x_val_ensemble, Y_val_t),\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the best  ensembling NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = file_prefix + r\"ensemble_hinge.best.hdf5\"\n",
    "ensemble_model = load_model(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble = ensemble_model.predict(x_test_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_NN_Ense_pred = []\n",
    "for row in y_pred_ensemble:\n",
    "    a = numpy.argmax(np.array(row)) #return the indices of the maximum values along the axis\n",
    "    Y_NN_Ense_pred.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_NN_Ense_pred_np = np.array(Y_NN_Ense_pred)\n",
    "private_set_y_np  = np.array(private_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = np.sum(Y_NN_Ense_pred_np == private_set_y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3589, 2565)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_NN_Ense_pred_np), count_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Hinge Loss Ensembling on Private Leader Board Data:0.7146837559208693\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Hinge Loss Ensembling on Private Leader Board Data:\"+str((float(count_correct)/len(Y_NN_Ense_pred_np))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Optimal Mean Squred Error (MSE) Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the training and testing data is already prepared in  9.2 we just need to train the model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define all the callback functions for the ensembling NN  model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback for saving the best ensembling NN model\n",
    "\n",
    "filepath = file_prefix + r\"ensemble_mse.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the ensembling NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model_generate():\n",
    "    nb_classes, nb_peturbations = 7, 8\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(nb_classes* nb_peturbations, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #categorical_hinge, squared_hinge, mean_squared_error,sparse_categorical_crossentropy\n",
    "    model.compile(loss='mean_squared_error',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1024)              58368     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 7)                 3591      \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,636,359\n",
      "Trainable params: 1,636,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del ensemble_model # does model exist in the current namespace\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "ensemble_model = ensemble_model_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the MSE ensembling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32181 samples, validate on 3589 samples\n",
      "Epoch 1/300\n",
      "32181/32181 [==============================] - 2s 59us/step - loss: 0.0356 - acc: 0.8503 - val_loss: 0.0616 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.71301\n",
      "Epoch 2/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0226 - acc: 0.8960 - val_loss: 0.0614 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.71301\n",
      "Epoch 3/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0217 - acc: 0.8985 - val_loss: 0.0618 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.71301\n",
      "Epoch 4/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0213 - acc: 0.8996 - val_loss: 0.0617 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.71301\n",
      "Epoch 5/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0210 - acc: 0.9008 - val_loss: 0.0618 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.71301\n",
      "Epoch 6/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0208 - acc: 0.9025 - val_loss: 0.0619 - val_acc: 0.7060\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.71301\n",
      "Epoch 7/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0207 - acc: 0.9026 - val_loss: 0.0622 - val_acc: 0.7060\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.71301\n",
      "Epoch 8/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0206 - acc: 0.9027 - val_loss: 0.0622 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.71301\n",
      "Epoch 9/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0205 - acc: 0.9028 - val_loss: 0.0623 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.71301\n",
      "Epoch 10/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0204 - acc: 0.9033 - val_loss: 0.0624 - val_acc: 0.7060\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.71301\n",
      "Epoch 11/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0203 - acc: 0.9034 - val_loss: 0.0625 - val_acc: 0.7052\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.71301\n",
      "Epoch 12/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0203 - acc: 0.9040 - val_loss: 0.0624 - val_acc: 0.7060\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.71301\n",
      "Epoch 13/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9041 - val_loss: 0.0625 - val_acc: 0.7069\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.71301\n",
      "Epoch 14/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0202 - acc: 0.9040 - val_loss: 0.0625 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.71301\n",
      "Epoch 15/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0201 - acc: 0.9045 - val_loss: 0.0627 - val_acc: 0.7074\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.71301\n",
      "Epoch 16/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0201 - acc: 0.9048 - val_loss: 0.0626 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.71301\n",
      "Epoch 17/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9050 - val_loss: 0.0627 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.71301\n",
      "Epoch 18/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9054 - val_loss: 0.0626 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.71301\n",
      "Epoch 19/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0200 - acc: 0.9052 - val_loss: 0.0627 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.71301\n",
      "Epoch 20/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9050 - val_loss: 0.0626 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.71301\n",
      "Epoch 21/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0199 - acc: 0.9051 - val_loss: 0.0628 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.71301\n",
      "Epoch 22/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0199 - acc: 0.9056 - val_loss: 0.0625 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.71301\n",
      "Epoch 23/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9055 - val_loss: 0.0630 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.71301\n",
      "Epoch 24/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9053 - val_loss: 0.0631 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.71301\n",
      "Epoch 25/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9059 - val_loss: 0.0631 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.71301\n",
      "Epoch 26/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0198 - acc: 0.9058 - val_loss: 0.0630 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.71301\n",
      "Epoch 27/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0198 - acc: 0.9057 - val_loss: 0.0629 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.71301\n",
      "Epoch 28/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9063 - val_loss: 0.0629 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.71301\n",
      "Epoch 29/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9064 - val_loss: 0.0631 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.71301\n",
      "Epoch 30/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9061 - val_loss: 0.0632 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.71301\n",
      "Epoch 31/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0197 - acc: 0.9066 - val_loss: 0.0629 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.71301 to 0.71329, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble_mse.best.hdf5\n",
      "Epoch 32/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0197 - acc: 0.9063 - val_loss: 0.0628 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.71329 to 0.71413, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble_mse.best.hdf5\n",
      "Epoch 33/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0196 - acc: 0.9069 - val_loss: 0.0631 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.71413\n",
      "Epoch 34/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0196 - acc: 0.9063 - val_loss: 0.0633 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.71413 to 0.71441, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble_mse.best.hdf5\n",
      "Epoch 35/300\n",
      "32181/32181 [==============================] - 1s 43us/step - loss: 0.0196 - acc: 0.9065 - val_loss: 0.0630 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.71441\n",
      "Epoch 36/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0196 - acc: 0.9070 - val_loss: 0.0631 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.71441\n",
      "Epoch 37/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0196 - acc: 0.9067 - val_loss: 0.0631 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.71441\n",
      "Epoch 38/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0196 - acc: 0.9067 - val_loss: 0.0631 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.71441\n",
      "Epoch 39/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0195 - acc: 0.9071 - val_loss: 0.0630 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.71441\n",
      "Epoch 40/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9073 - val_loss: 0.0631 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.71441\n",
      "Epoch 41/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9069 - val_loss: 0.0632 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.71441\n",
      "Epoch 42/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9069 - val_loss: 0.0632 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.71441\n",
      "Epoch 43/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9072 - val_loss: 0.0630 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.71441\n",
      "Epoch 44/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9072 - val_loss: 0.0630 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.71441\n",
      "Epoch 45/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9075 - val_loss: 0.0631 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.71441\n",
      "Epoch 46/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0194 - acc: 0.9076 - val_loss: 0.0633 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.71441\n",
      "Epoch 47/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0194 - acc: 0.9069 - val_loss: 0.0634 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.71441\n",
      "Epoch 48/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0194 - acc: 0.9077 - val_loss: 0.0633 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.71441\n",
      "Epoch 49/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0194 - acc: 0.9076 - val_loss: 0.0633 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.71441\n",
      "Epoch 50/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0194 - acc: 0.9073 - val_loss: 0.0634 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.71441\n",
      "Epoch 51/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0194 - acc: 0.9074 - val_loss: 0.0631 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.71441\n",
      "Epoch 52/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0194 - acc: 0.9074 - val_loss: 0.0633 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.71441\n",
      "Epoch 53/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0193 - acc: 0.9079 - val_loss: 0.0632 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.71441\n",
      "Epoch 54/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0193 - acc: 0.9072 - val_loss: 0.0634 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.71441\n",
      "Epoch 55/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0193 - acc: 0.9077 - val_loss: 0.0632 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.71441 to 0.71496, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble_mse.best.hdf5\n",
      "Epoch 56/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0193 - acc: 0.9077 - val_loss: 0.0632 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.71496\n",
      "Epoch 57/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0193 - acc: 0.9075 - val_loss: 0.0633 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.71496\n",
      "Epoch 58/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0193 - acc: 0.9081 - val_loss: 0.0634 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.71496\n",
      "Epoch 59/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0193 - acc: 0.9084 - val_loss: 0.0633 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.71496\n",
      "Epoch 60/300\n",
      "32181/32181 [==============================] - 1s 43us/step - loss: 0.0193 - acc: 0.9082 - val_loss: 0.0631 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.71496\n",
      "Epoch 61/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0193 - acc: 0.9079 - val_loss: 0.0635 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.71496\n",
      "Epoch 62/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9081 - val_loss: 0.0632 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.71496\n",
      "Epoch 63/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9083 - val_loss: 0.0634 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.71496\n",
      "Epoch 64/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9079 - val_loss: 0.0632 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.71496\n",
      "Epoch 65/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9084 - val_loss: 0.0633 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.71496\n",
      "Epoch 66/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9078 - val_loss: 0.0634 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.71496\n",
      "Epoch 67/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9085 - val_loss: 0.0634 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.71496\n",
      "Epoch 68/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9081 - val_loss: 0.0634 - val_acc: 0.7155\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.71496 to 0.71552, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble_mse.best.hdf5\n",
      "Epoch 69/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9084 - val_loss: 0.0636 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.71552\n",
      "Epoch 70/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0192 - acc: 0.9085 - val_loss: 0.0634 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.71552\n",
      "Epoch 71/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9088 - val_loss: 0.0636 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.71552\n",
      "Epoch 72/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9083 - val_loss: 0.0632 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.71552\n",
      "Epoch 73/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9085 - val_loss: 0.0634 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.71552\n",
      "Epoch 74/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9085 - val_loss: 0.0635 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.71552\n",
      "Epoch 75/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9085 - val_loss: 0.0635 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.71552\n",
      "Epoch 76/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9086 - val_loss: 0.0637 - val_acc: 0.7155\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.71552\n",
      "Epoch 77/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9089 - val_loss: 0.0636 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.71552\n",
      "Epoch 78/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9085 - val_loss: 0.0634 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.71552\n",
      "Epoch 79/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9087 - val_loss: 0.0636 - val_acc: 0.7152\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.71552\n",
      "Epoch 80/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0191 - acc: 0.9089 - val_loss: 0.0637 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.71552\n",
      "Epoch 81/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9089 - val_loss: 0.0637 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.71552\n",
      "Epoch 82/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9082 - val_loss: 0.0637 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.71552\n",
      "Epoch 83/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9091 - val_loss: 0.0636 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.71552\n",
      "Epoch 84/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0190 - acc: 0.9086 - val_loss: 0.0636 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.71552\n",
      "Epoch 85/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9090 - val_loss: 0.0638 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.71552\n",
      "Epoch 86/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9094 - val_loss: 0.0636 - val_acc: 0.7158\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.71552 to 0.71580, saving model to FaceExp_Reco_tr_09_v2_v02_ensemble_mse.best.hdf5\n",
      "Epoch 87/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9092 - val_loss: 0.0635 - val_acc: 0.7158\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.71580\n",
      "Epoch 88/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9091 - val_loss: 0.0637 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.71580\n",
      "Epoch 89/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9088 - val_loss: 0.0636 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.71580\n",
      "Epoch 90/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0190 - acc: 0.9095 - val_loss: 0.0636 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.71580\n",
      "Epoch 91/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9093 - val_loss: 0.0638 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.71580\n",
      "Epoch 92/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9100 - val_loss: 0.0637 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.71580\n",
      "Epoch 93/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9100 - val_loss: 0.0638 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.71580\n",
      "Epoch 94/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9093 - val_loss: 0.0636 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.71580\n",
      "Epoch 95/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9094 - val_loss: 0.0635 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.71580\n",
      "Epoch 96/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9096 - val_loss: 0.0636 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.71580\n",
      "Epoch 97/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9097 - val_loss: 0.0639 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.71580\n",
      "Epoch 98/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9098 - val_loss: 0.0638 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.71580\n",
      "Epoch 99/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9095 - val_loss: 0.0640 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.71580\n",
      "Epoch 100/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9095 - val_loss: 0.0639 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.71580\n",
      "Epoch 101/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0189 - acc: 0.9098 - val_loss: 0.0638 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.71580\n",
      "Epoch 102/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9093 - val_loss: 0.0636 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.71580\n",
      "Epoch 103/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9094 - val_loss: 0.0637 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.71580\n",
      "Epoch 104/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9099 - val_loss: 0.0637 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.71580\n",
      "Epoch 105/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0188 - acc: 0.9101 - val_loss: 0.0640 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.71580\n",
      "Epoch 106/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9105 - val_loss: 0.0636 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.71580\n",
      "Epoch 107/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0188 - acc: 0.9102 - val_loss: 0.0638 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.71580\n",
      "Epoch 108/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9097 - val_loss: 0.0637 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.71580\n",
      "Epoch 109/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9102 - val_loss: 0.0636 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.71580\n",
      "Epoch 110/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0188 - acc: 0.9101 - val_loss: 0.0639 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.71580\n",
      "Epoch 111/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0187 - acc: 0.9098 - val_loss: 0.0638 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.71580\n",
      "Epoch 112/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9103 - val_loss: 0.0637 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.71580\n",
      "Epoch 113/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9102 - val_loss: 0.0641 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.71580\n",
      "Epoch 114/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0187 - acc: 0.9101 - val_loss: 0.0636 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.71580\n",
      "Epoch 115/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9104 - val_loss: 0.0638 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.71580\n",
      "Epoch 116/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9108 - val_loss: 0.0638 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.71580\n",
      "Epoch 117/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9112 - val_loss: 0.0638 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.71580\n",
      "Epoch 118/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9104 - val_loss: 0.0640 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.71580\n",
      "Epoch 119/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9107 - val_loss: 0.0638 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.71580\n",
      "Epoch 120/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9104 - val_loss: 0.0638 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.71580\n",
      "Epoch 121/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0187 - acc: 0.9111 - val_loss: 0.0637 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.71580\n",
      "Epoch 122/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0186 - acc: 0.9108 - val_loss: 0.0641 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.71580\n",
      "Epoch 123/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9108 - val_loss: 0.0637 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.71580\n",
      "Epoch 124/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9114 - val_loss: 0.0638 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.71580\n",
      "Epoch 125/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9113 - val_loss: 0.0638 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.71580\n",
      "Epoch 126/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9109 - val_loss: 0.0637 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.71580\n",
      "Epoch 127/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9111 - val_loss: 0.0638 - val_acc: 0.7122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00127: val_acc did not improve from 0.71580\n",
      "Epoch 128/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0186 - acc: 0.9113 - val_loss: 0.0641 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.71580\n",
      "Epoch 129/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0186 - acc: 0.9115 - val_loss: 0.0642 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.71580\n",
      "Epoch 130/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9110 - val_loss: 0.0638 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.71580\n",
      "Epoch 131/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0186 - acc: 0.9114 - val_loss: 0.0640 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.71580\n",
      "Epoch 132/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9110 - val_loss: 0.0637 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.71580\n",
      "Epoch 133/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9111 - val_loss: 0.0640 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.71580\n",
      "Epoch 134/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9113 - val_loss: 0.0639 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.71580\n",
      "Epoch 135/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0185 - acc: 0.9112 - val_loss: 0.0638 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.71580\n",
      "Epoch 136/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9111 - val_loss: 0.0638 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.71580\n",
      "Epoch 137/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0185 - acc: 0.9115 - val_loss: 0.0641 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.71580\n",
      "Epoch 138/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9113 - val_loss: 0.0639 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.71580\n",
      "Epoch 139/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9115 - val_loss: 0.0638 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.71580\n",
      "Epoch 140/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9113 - val_loss: 0.0637 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.71580\n",
      "Epoch 141/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9120 - val_loss: 0.0640 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.71580\n",
      "Epoch 142/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9116 - val_loss: 0.0642 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.71580\n",
      "Epoch 143/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9120 - val_loss: 0.0641 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.71580\n",
      "Epoch 144/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9117 - val_loss: 0.0642 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.71580\n",
      "Epoch 145/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9116 - val_loss: 0.0638 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.71580\n",
      "Epoch 146/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9122 - val_loss: 0.0642 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.71580\n",
      "Epoch 147/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0184 - acc: 0.9122 - val_loss: 0.0640 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.71580\n",
      "Epoch 148/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9118 - val_loss: 0.0638 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.71580\n",
      "Epoch 149/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9123 - val_loss: 0.0642 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.71580\n",
      "Epoch 150/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9122 - val_loss: 0.0640 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.71580\n",
      "Epoch 151/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0184 - acc: 0.9121 - val_loss: 0.0641 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.71580\n",
      "Epoch 152/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0183 - acc: 0.9125 - val_loss: 0.0639 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.71580\n",
      "Epoch 153/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0183 - acc: 0.9125 - val_loss: 0.0639 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.71580\n",
      "Epoch 154/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9127 - val_loss: 0.0642 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.71580\n",
      "Epoch 155/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9125 - val_loss: 0.0639 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.71580\n",
      "Epoch 156/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9130 - val_loss: 0.0642 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.71580\n",
      "Epoch 157/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9126 - val_loss: 0.0639 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.71580\n",
      "Epoch 158/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0183 - acc: 0.9123 - val_loss: 0.0643 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.71580\n",
      "Epoch 159/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9127 - val_loss: 0.0640 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.71580\n",
      "Epoch 160/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0183 - acc: 0.9127 - val_loss: 0.0642 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.71580\n",
      "Epoch 161/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9128 - val_loss: 0.0640 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.71580\n",
      "Epoch 162/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9129 - val_loss: 0.0639 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.71580\n",
      "Epoch 163/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9130 - val_loss: 0.0642 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.71580\n",
      "Epoch 164/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9131 - val_loss: 0.0641 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.71580\n",
      "Epoch 165/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9127 - val_loss: 0.0640 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.71580\n",
      "Epoch 166/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9133 - val_loss: 0.0640 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.71580\n",
      "Epoch 167/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9133 - val_loss: 0.0642 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.71580\n",
      "Epoch 168/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0182 - acc: 0.9129 - val_loss: 0.0638 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.71580\n",
      "Epoch 169/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9136 - val_loss: 0.0642 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.71580\n",
      "Epoch 170/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9130 - val_loss: 0.0644 - val_acc: 0.7122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00170: val_acc did not improve from 0.71580\n",
      "Epoch 171/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9132 - val_loss: 0.0644 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.71580\n",
      "Epoch 172/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9135 - val_loss: 0.0645 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.71580\n",
      "Epoch 173/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9134 - val_loss: 0.0642 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.71580\n",
      "Epoch 174/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9133 - val_loss: 0.0641 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.71580\n",
      "Epoch 175/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0181 - acc: 0.9136 - val_loss: 0.0643 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.71580\n",
      "Epoch 176/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0181 - acc: 0.9134 - val_loss: 0.0640 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.71580\n",
      "Epoch 177/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9139 - val_loss: 0.0642 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.71580\n",
      "Epoch 178/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9138 - val_loss: 0.0642 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.71580\n",
      "Epoch 179/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9143 - val_loss: 0.0641 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.71580\n",
      "Epoch 180/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9145 - val_loss: 0.0643 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.71580\n",
      "Epoch 181/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9141 - val_loss: 0.0641 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.71580\n",
      "Epoch 182/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9140 - val_loss: 0.0641 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.71580\n",
      "Epoch 183/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0180 - acc: 0.9142 - val_loss: 0.0639 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.71580\n",
      "Epoch 184/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9143 - val_loss: 0.0640 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.71580\n",
      "Epoch 185/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0180 - acc: 0.9141 - val_loss: 0.0645 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.71580\n",
      "Epoch 186/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9143 - val_loss: 0.0640 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.71580\n",
      "Epoch 187/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9145 - val_loss: 0.0645 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.71580\n",
      "Epoch 188/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9145 - val_loss: 0.0644 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.71580\n",
      "Epoch 189/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0179 - acc: 0.9144 - val_loss: 0.0644 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.71580\n",
      "Epoch 190/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9145 - val_loss: 0.0644 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.71580\n",
      "Epoch 191/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9148 - val_loss: 0.0645 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.71580\n",
      "Epoch 192/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0179 - acc: 0.9149 - val_loss: 0.0642 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.71580\n",
      "Epoch 193/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0178 - acc: 0.9149 - val_loss: 0.0646 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.71580\n",
      "Epoch 194/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9144 - val_loss: 0.0646 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.71580\n",
      "Epoch 195/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9150 - val_loss: 0.0644 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.71580\n",
      "Epoch 196/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9152 - val_loss: 0.0643 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.71580\n",
      "Epoch 197/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9154 - val_loss: 0.0643 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.71580\n",
      "Epoch 198/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9150 - val_loss: 0.0645 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.71580\n",
      "Epoch 199/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9150 - val_loss: 0.0644 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.71580\n",
      "Epoch 200/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0178 - acc: 0.9157 - val_loss: 0.0646 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.71580\n",
      "Epoch 201/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0177 - acc: 0.9150 - val_loss: 0.0643 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.71580\n",
      "Epoch 202/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0177 - acc: 0.9152 - val_loss: 0.0641 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.71580\n",
      "Epoch 203/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0177 - acc: 0.9156 - val_loss: 0.0643 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.71580\n",
      "Epoch 204/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0177 - acc: 0.9156 - val_loss: 0.0645 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.71580\n",
      "Epoch 205/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0177 - acc: 0.9157 - val_loss: 0.0641 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.71580\n",
      "Epoch 206/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0177 - acc: 0.9163 - val_loss: 0.0646 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.71580\n",
      "Epoch 207/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9159 - val_loss: 0.0644 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.71580\n",
      "Epoch 208/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9163 - val_loss: 0.0646 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.71580\n",
      "Epoch 209/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9163 - val_loss: 0.0647 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.71580\n",
      "Epoch 210/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9165 - val_loss: 0.0644 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.71580\n",
      "Epoch 211/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9158 - val_loss: 0.0646 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.71580\n",
      "Epoch 212/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9163 - val_loss: 0.0643 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.71580\n",
      "Epoch 213/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9166 - val_loss: 0.0643 - val_acc: 0.7122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00213: val_acc did not improve from 0.71580\n",
      "Epoch 214/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0176 - acc: 0.9161 - val_loss: 0.0647 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.71580\n",
      "Epoch 215/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0175 - acc: 0.9167 - val_loss: 0.0646 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.71580\n",
      "Epoch 216/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0175 - acc: 0.9172 - val_loss: 0.0647 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.71580\n",
      "Epoch 217/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0175 - acc: 0.9167 - val_loss: 0.0643 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.71580\n",
      "Epoch 218/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0175 - acc: 0.9164 - val_loss: 0.0647 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.71580\n",
      "Epoch 219/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0175 - acc: 0.9168 - val_loss: 0.0645 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.71580\n",
      "Epoch 220/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9170 - val_loss: 0.0643 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.71580\n",
      "Epoch 221/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9170 - val_loss: 0.0647 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.71580\n",
      "Epoch 222/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9171 - val_loss: 0.0642 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.71580\n",
      "Epoch 223/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9173 - val_loss: 0.0647 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.71580\n",
      "Epoch 224/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9170 - val_loss: 0.0644 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.71580\n",
      "Epoch 225/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9174 - val_loss: 0.0645 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.71580\n",
      "Epoch 226/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9175 - val_loss: 0.0645 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.71580\n",
      "Epoch 227/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0174 - acc: 0.9177 - val_loss: 0.0648 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.71580\n",
      "Epoch 228/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0173 - acc: 0.9177 - val_loss: 0.0648 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.71580\n",
      "Epoch 229/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0173 - acc: 0.9174 - val_loss: 0.0646 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.71580\n",
      "Epoch 230/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0173 - acc: 0.9176 - val_loss: 0.0649 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.71580\n",
      "Epoch 231/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0173 - acc: 0.9177 - val_loss: 0.0650 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.71580\n",
      "Epoch 232/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0173 - acc: 0.9184 - val_loss: 0.0645 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.71580\n",
      "Epoch 233/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0173 - acc: 0.9180 - val_loss: 0.0647 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.71580\n",
      "Epoch 234/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0172 - acc: 0.9182 - val_loss: 0.0649 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.71580\n",
      "Epoch 235/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0172 - acc: 0.9181 - val_loss: 0.0651 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.71580\n",
      "Epoch 236/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0172 - acc: 0.9187 - val_loss: 0.0646 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.71580\n",
      "Epoch 237/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0172 - acc: 0.9189 - val_loss: 0.0646 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.71580\n",
      "Epoch 238/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0172 - acc: 0.9182 - val_loss: 0.0646 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.71580\n",
      "Epoch 239/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0171 - acc: 0.9187 - val_loss: 0.0654 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.71580\n",
      "Epoch 240/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0172 - acc: 0.9186 - val_loss: 0.0650 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.71580\n",
      "Epoch 241/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0171 - acc: 0.9187 - val_loss: 0.0645 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.71580\n",
      "Epoch 242/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0171 - acc: 0.9190 - val_loss: 0.0652 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.71580\n",
      "Epoch 243/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0171 - acc: 0.9187 - val_loss: 0.0650 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.71580\n",
      "Epoch 244/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0171 - acc: 0.9191 - val_loss: 0.0649 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.71580\n",
      "Epoch 245/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0170 - acc: 0.9194 - val_loss: 0.0645 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.71580\n",
      "Epoch 246/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0170 - acc: 0.9190 - val_loss: 0.0647 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.71580\n",
      "Epoch 247/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0170 - acc: 0.9197 - val_loss: 0.0643 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.71580\n",
      "Epoch 248/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0170 - acc: 0.9195 - val_loss: 0.0647 - val_acc: 0.7119\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.71580\n",
      "Epoch 249/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0170 - acc: 0.9200 - val_loss: 0.0649 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.71580\n",
      "Epoch 250/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0170 - acc: 0.9195 - val_loss: 0.0647 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.71580\n",
      "Epoch 251/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0169 - acc: 0.9197 - val_loss: 0.0647 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.71580\n",
      "Epoch 252/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0169 - acc: 0.9201 - val_loss: 0.0648 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.71580\n",
      "Epoch 253/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0169 - acc: 0.9197 - val_loss: 0.0653 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.71580\n",
      "Epoch 254/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0169 - acc: 0.9198 - val_loss: 0.0650 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.71580\n",
      "Epoch 255/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0169 - acc: 0.9201 - val_loss: 0.0651 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.71580\n",
      "Epoch 256/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0169 - acc: 0.9205 - val_loss: 0.0648 - val_acc: 0.7102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00256: val_acc did not improve from 0.71580\n",
      "Epoch 257/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0168 - acc: 0.9209 - val_loss: 0.0661 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.71580\n",
      "Epoch 258/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0168 - acc: 0.9206 - val_loss: 0.0653 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.71580\n",
      "Epoch 259/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0168 - acc: 0.9217 - val_loss: 0.0650 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.71580\n",
      "Epoch 260/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0168 - acc: 0.9210 - val_loss: 0.0649 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.71580\n",
      "Epoch 261/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0168 - acc: 0.9208 - val_loss: 0.0651 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.71580\n",
      "Epoch 262/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0167 - acc: 0.9209 - val_loss: 0.0653 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.71580\n",
      "Epoch 263/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0167 - acc: 0.9210 - val_loss: 0.0650 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.71580\n",
      "Epoch 264/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0167 - acc: 0.9211 - val_loss: 0.0651 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.71580\n",
      "Epoch 265/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0167 - acc: 0.9214 - val_loss: 0.0651 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.71580\n",
      "Epoch 266/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0167 - acc: 0.9215 - val_loss: 0.0654 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.71580\n",
      "Epoch 267/300\n",
      "32181/32181 [==============================] - 1s 42us/step - loss: 0.0166 - acc: 0.9216 - val_loss: 0.0651 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.71580\n",
      "Epoch 268/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0166 - acc: 0.9218 - val_loss: 0.0649 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.71580\n",
      "Epoch 269/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0166 - acc: 0.9213 - val_loss: 0.0654 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.71580\n",
      "Epoch 270/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0166 - acc: 0.9217 - val_loss: 0.0649 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.71580\n",
      "Epoch 271/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0166 - acc: 0.9222 - val_loss: 0.0653 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.71580\n",
      "Epoch 272/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0166 - acc: 0.9221 - val_loss: 0.0649 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.71580\n",
      "Epoch 273/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0165 - acc: 0.9223 - val_loss: 0.0647 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.71580\n",
      "Epoch 274/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0165 - acc: 0.9225 - val_loss: 0.0651 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.71580\n",
      "Epoch 275/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0165 - acc: 0.9226 - val_loss: 0.0657 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.71580\n",
      "Epoch 276/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0165 - acc: 0.9228 - val_loss: 0.0655 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.71580\n",
      "Epoch 277/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0165 - acc: 0.9224 - val_loss: 0.0653 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.71580\n",
      "Epoch 278/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0164 - acc: 0.9237 - val_loss: 0.0651 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.71580\n",
      "Epoch 279/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0164 - acc: 0.9229 - val_loss: 0.0654 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.71580\n",
      "Epoch 280/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0164 - acc: 0.9233 - val_loss: 0.0653 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.71580\n",
      "Epoch 281/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0164 - acc: 0.9231 - val_loss: 0.0653 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.71580\n",
      "Epoch 282/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0163 - acc: 0.9236 - val_loss: 0.0653 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.71580\n",
      "Epoch 283/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0163 - acc: 0.9232 - val_loss: 0.0654 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.71580\n",
      "Epoch 284/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0163 - acc: 0.9234 - val_loss: 0.0654 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.71580\n",
      "Epoch 285/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0163 - acc: 0.9242 - val_loss: 0.0648 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.71580\n",
      "Epoch 286/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0163 - acc: 0.9241 - val_loss: 0.0652 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.71580\n",
      "Epoch 287/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0162 - acc: 0.9242 - val_loss: 0.0655 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.71580\n",
      "Epoch 288/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0162 - acc: 0.9234 - val_loss: 0.0654 - val_acc: 0.7111\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.71580\n",
      "Epoch 289/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0162 - acc: 0.9241 - val_loss: 0.0652 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.71580\n",
      "Epoch 290/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0162 - acc: 0.9246 - val_loss: 0.0657 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.71580\n",
      "Epoch 291/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0162 - acc: 0.9244 - val_loss: 0.0657 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.71580\n",
      "Epoch 292/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0162 - acc: 0.9247 - val_loss: 0.0659 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.71580\n",
      "Epoch 293/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0161 - acc: 0.9248 - val_loss: 0.0655 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.71580\n",
      "Epoch 294/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0161 - acc: 0.9246 - val_loss: 0.0651 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.71580\n",
      "Epoch 295/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0161 - acc: 0.9250 - val_loss: 0.0654 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.71580\n",
      "Epoch 296/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0160 - acc: 0.9249 - val_loss: 0.0653 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.71580\n",
      "Epoch 297/300\n",
      "32181/32181 [==============================] - 1s 40us/step - loss: 0.0160 - acc: 0.9250 - val_loss: 0.0659 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.71580\n",
      "Epoch 298/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0160 - acc: 0.9260 - val_loss: 0.0656 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.71580\n",
      "Epoch 299/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0160 - acc: 0.9252 - val_loss: 0.0654 - val_acc: 0.7105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00299: val_acc did not improve from 0.71580\n",
      "Epoch 300/300\n",
      "32181/32181 [==============================] - 1s 41us/step - loss: 0.0160 - acc: 0.9262 - val_loss: 0.0661 - val_acc: 0.7105\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.71580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbea8773748>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model.fit(x_train_ensemble, Y_train_t,\n",
    "          batch_size=128,\n",
    "          epochs=300,\n",
    "          verbose=1,\n",
    "          class_weight=class_weights,\n",
    "          validation_data=(x_val_ensemble, Y_val_t),\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = file_prefix + r\"ensemble_mse.best.hdf5\"\n",
    "ensemble_model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble = ensemble_model.predict(x_test_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_NN_Ense_pred = []\n",
    "for row in y_pred_ensemble:\n",
    "    a = numpy.argmax(np.array(row)) #return the indices of the maximum values along the axis\n",
    "    Y_NN_Ense_pred.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_NN_Ense_pred_np = np.array(Y_NN_Ense_pred)\n",
    "private_set_y_np  = np.array(private_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3589, 2565)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_NN_Ense_pred_np), count_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Mean Square Error Ensembling on Private Leader Board Data:0.7146837559208693\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Mean Square Error Ensembling on Private Leader Board Data:\"+str((float(count_correct)/len(Y_NN_Ense_pred_np))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.  Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Conclusion is written on the top in Cell **2 Conclusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
